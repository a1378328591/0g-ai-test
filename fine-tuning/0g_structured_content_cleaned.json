[
  {
    "menu": "Understanding 0G",
    "sections": [
      {
        "title": "Why 0G Exists‚Äã",
        "content": "AI (Artificial Intelligence) is rapidly advancing, but its powerful capabilities are largely confined to centralized systems & limited to a few large companies. Bringing AI onto the blockchain unlocks transformative potential: truly verifiable AI, user-owned data powering AI applications, and open, censorship-resistant AI development. However, a fundamental challenge has held back this vision: AI's Data Hunger: AI models and datasets are massive. Existing blockchains make storing and accessing this data impossibly expensive and slow. Intense Compute Demands: AI requires significant processing power, far beyond what traditional blockchains can offer efficiently. Need for Speed: Real-time AI applications demand high throughput and low latency Without overcoming these hurdles, the dream of decentralized AI remains out of reach. 0G is built to break these barriers. We provide the foundational infrastructure like high-performance storage, scalable compute, and a fast, modular blockchain‚Äîdesigned from the ground up to power the future of on-chain AI."
      },
      {
        "title": "What is 0G?‚Äã",
        "content": "0G (Zero Gravity) is the first decentralized AI L1 chain that orchestrates hardware resources (storage, compute) and software assets (data, models) to handle AI workloads at scale. It bridges the gap between Web2 AI capabilities and Web3 decentralization. 0G IS BUILDING THE GLOBAL FOUNDATION FOR A BETTER, FAIRER, AND MORE OPEN AI ECOSYSTEM, WHERE POWER IS DISTRIBUTED AND INNOVATION THRIVES How it works: 0G provides four independent services that solve different pieces of the AI + blockchain puzzle: Storage ‚Üí Where to keep massive AI datasets Compute ‚Üí How to run AI models economically Chain ‚Üí Where to execute AI transactions quickly Data Availability ‚Üí How to ensure data is always accessible"
      },
      {
        "title": "Modular Architecture‚Äã",
        "content": "KEY BENEFIT OF MODULAR ARCHITECTURE: YOU DON'T NEED TO USE ALL OF 0G! Pick only what you need: Already on Ethereum, Polygon, or any EVM chain? Use 0G Storage and Compute directly from your existing smart contracts, no need to migrate. Building on Solana or other non-EVM chains? Our SDKs support cross-chain integration Just need one service? Use only 0G Storage or only 0G Compute"
      },
      {
        "title": "The 4 Components of 0G‚Äã",
        "content": "Component Works Independently? Key Features & Use Cases Cost Highlight 0G Chain ‚úÖ Yes (Optional for other services) Fastest modular EVM L1 for AI agents, DeFi with AI logic Low gas fees in 0G token 0G Storage ‚úÖ Yes (Any app/chain can access) Store AI models (GBs-TBs), training datasets, user files, game assets 10-100x cheaper than alternatives 0G Compute ‚úÖ Yes (Any app/chain can access) Run AI inference, model training, verifiable compute, ML pipelines Pay-only-for-what-you-use 0G DA ‚úÖ Yes (Works with any rollup/L1/L2) Power gaming chains, AI rollups, high-frequency trading chains Economical for high-volume DA *0G Storage can be used completely standalone without any blockchain integration - perfect for traditional apps needing decentralized storage."
      },
      {
        "title": "Key Concepts Explained Simply‚Äã",
        "content": "What is decentralized storage? What is data availability? What is an AI compute network? What is a modular blockchain?"
      },
      {
        "title": "Why Zero Gravity?‚Äã",
        "content": "0G represents Zero Gravity - the state where everything flows effortlessly. Just as astronauts move freely in zero gravity, data and AI computations flow seamlessly through our network without the heavy \"gravity\" of: High costs Slow speeds Technical barriers Platform lock-in"
      },
      {
        "title": "What Can You Build?‚Äã",
        "content": "With 0G's technology, previously impossible use cases are now within reach: On-chain AI agents that learn and evolve Decentralized ChatGPT alternatives AI-powered DeFi trading systems Medical AI with patient-owned data Large-scale ML training without AWS bills And this is just the beginning."
      },
      {
        "title": "Where to Go Next‚Äã",
        "content": "Now that you understand what 0G is and why it exists, here's how to dive deeper: For Learners ‚Üí Read more about Concepts to understand how each component works For Builders ‚Üí Jump into the Developer Hub to start building For Operators ‚Üí Learn how to Run a Node and earn rewards"
      },
      {
        "title": "Join the 0G Community‚Äã",
        "content": "Discord - Get help and chat with builders X(Twitter) - Latest updates and announcements GitHub - Contribute to the project We're excited to have you on board as we build the future of Web3 √ó AI together!"
      }
    ]
  },
  {
    "menu": "Vision & Mission",
    "sections": [
      {
        "title": "Our Mission: Make AI a Public Good‚Äã",
        "content": "At 0G, our mission is clear: To Make AI a Public Good. We believe that AI technology should be accessible, transparent, and beneficial to everyone, not just a select few. By building a decentralized AI operating system, we're creating the infrastructure that will enable this vision."
      },
      {
        "title": "Our Vision‚Äã",
        "content": "We envision a world where: AI is democratized: Anyone can access and contribute to AI development without gatekeepers AI is transparent: The models, data, and processes are open and verifiable AI is fair: Resources and benefits are distributed equitably across the network AI is secure: Decentralization ensures no single point of failure or control"
      },
      {
        "title": "How We Achieve This‚Äã",
        "content": "Every component of our ecosystem contributes toward this goal: Open Infrastructure: By providing decentralized storage, compute, and data availability, we remove the barriers to AI development Community Ownership: Through our node network and governance model, the community owns and operates the infrastructure Economic Alignment: Our tokenomics ensure that contributors are fairly rewarded for their participation Technical Excellence: We build the fastest, most efficient infrastructure to make decentralized AI competitive with centralized alternatives"
      },
      {
        "title": "Join Our Mission‚Äã",
        "content": "We invite you to join us in building the foundation for a decentralized AI future. Whether you're a developer, node operator, or community member, there's a place for you in the 0G ecosystem. Together, we're not just building technology ‚Äì we're shaping the future of AI for the benefit of all humanity."
      },
      {
        "title": "Join the 0G Community‚Äã",
        "content": "Discord X(Twitter) GitHub"
      }
    ]
  },
  {
    "menu": "0G Chain: The Fastest Modular AI Chain",
    "sections": [
      {
        "title": "The Problem with AI on Blockchain‚Äã",
        "content": "Try running an AI model on Ethereum today: Cost: $1M+ in gas fees for a simple model Speed: 15 transactions per second (AI needs thousands) Data: Can't handle AI's massive data requirements"
      },
      {
        "title": "What is 0G Chain?‚Äã",
        "content": "0G Chain is a blockchain built specifically for AI applications. Think of it as Ethereum, but optimized for AI workloads with significantly higher throughput. EVM COMPATIBILITY Your existing Ethereum code works without changes ü§ù"
      },
      {
        "title": "How 0G Chain Works‚Äã",
        "content": "Modular Architecture‚Äã 0G Chain features an advanced modular design that distinctly separates consensus from execution. This separation into independent, yet interconnected, layers is a cornerstone of 0G Chain's architecture, delivering enhanced flexibility, scalability, and a faster pace of innovation. Architecture Overview: Consensus Layer: Dedicated to achieving network agreement. It manages validator coordination, block production, and ensures the overall security and finality of the chain. Execution Layer: Focused on state management. It handles smart contract execution, processes transactions, and maintains compatibility with the EVM (Ethereum Virtual Machine). Key Technical Advantages: Independent Upgradability: The execution layer can rapidly incorporate new EVM features (such as EIP-4844, account abstraction, or novel opcodes) without requiring changes to the underlying consensus mechanism. Focused Optimization: Conversely, the consensus layer can be upgraded with critical performance or security enhancements without impacting the EVM or ongoing execution processes. Accelerated Development: This decoupling allows for parallel development and faster iteration cycles for both layers, leading to quicker adoption of new technologies and improvements in both performance and features. This design makes 0G Chain flexible and fast. When new blockchain features come out, we can add them quickly without breaking anything. This keeps 0G optimized for AI while staying up-to-date with the latest technology. Optimized Consensus‚Äã 0G Chain employs a highly optimized version of CometBFT (formerly Tendermint) as its consensus mechanism, with meticulously tuned parameters that achieve maximum performance while maintaining security. The system features carefully calibrated block production intervals and timeout configurations that work together to deliver high throughput, ensure network stability, and enable faster consensus rounds‚Äîall without compromising the fundamental safety guarantees. These optimizations enable 0G Chain to achieve maximum performance: 2,500+ TPS: Current throughput significantly exceeds traditional blockchain networks Sub-second Finality: Near-instant transaction confirmation for AI applications Consistent Performance: Maintains high throughput even under heavy network load Scaling Roadmap‚Äã DAG-Based Consensus: Transitioning to Directed Acyclic Graph (DAG) based consensus for exponentially higher efficiency Parallel transaction processing capabilities Elimination of sequential block limitations Shared Security Model: Implementing shared staking mechanisms to enhance network security Validators can secure multiple services simultaneously Increased capital efficiency for stakers"
      },
      {
        "title": "Technical Deep Dive‚Äã",
        "content": "How does 0G achieve high throughput? How does the validator system work? What makes 0G different from other fast chains? 0G Chain's modular architecture enables seamless integration with storage, compute, and DA layers"
      },
      {
        "title": "Validator Participation‚Äã",
        "content": "Validators earn rewards through: Block rewards: For producing valid blocks Transaction fees: From network usage Staking rewards: Based on stake size and uptime Validator reward and penalty structure in the 0G network"
      },
      {
        "title": "Frequently Asked Questions‚Äã",
        "content": "Is 0G Chain truly decentralized? Do I need to rewrite my Ethereum dApp? Why is it faster than Ethereum?"
      },
      {
        "title": "Next Steps‚Äã",
        "content": "Ready to build? Start here: Quick Start Guide - Deploy in 5 minutes Migration from Ethereum - Move existing dApps Technical Whitepaper - Deep architecture details 0G Chain: Where AI meets blockchain at scale."
      }
    ]
  },
  {
    "menu": "0G DA: Infinitely Scalable and Programmable Data Availability",
    "sections": [
      {
        "title": "The Rise of Data Availability Layers‚Äã",
        "content": "Data availability (DA) refers to proving that data is readily accessible, verifiable, and retrievable. For example, Layer 2 rollups such as Arbitrum or Base reduce the burden on Ethereum by handling transactions off-chain and then publishing the data back to Ethereum, thereby freeing up L1 throughput and reducing gas costs. The transaction data, however, still needs to be made available so that anyone can validate or challenge the transactions through fraud proofs during the challenge period. As such, DA is crucial to blockchains as it allows for full validation of the blockchain's history and current state by any participant, thus maintaining the decentralized and trustless nature of the network. Without this, validators would not be able to independently verify the legitimacy of transactions and blocks, leading to potential issues like fraud or censorship. This led to the emergence of Data Availability Layers (DALs), which provide a significantly more efficient manner of storing and verifying data than publishing directly to Ethereum. DALs are critical for several reasons: Scalability: DALs allow networks to process more transactions and larger datasets without overwhelming the system, reducing the burden on network nodes and significantly enhancing network scalability. Increased Efficiency: DALs optimize how and where data is stored and made available, increasing data throughput and reducing latency while also minimizing associated costs. Interoperability & Innovation: DALs that can interact with multiple ecosystems enable fast and highly secure interoperability for data and assets. However, it's worth noting that not all DALs are built equally."
      },
      {
        "title": "The Challenge Today‚Äã",
        "content": "Existing DALs tend to require that data be simultaneously sent to all of their network nodes, preventing horizontal scalability and limiting network speed to its slowest node. They also do not have built-in storage systems, requiring connectivity to external systems that impact throughput, latency, and cost. Additionally, 0G inherits Ethereum's security, while other systems rely upon their own security mechanisms that fall short. This is significant because Ethereum's network is secured by over 34 million ETH staked, representing approximately $80 billion in cryptoeconomic security at the time of writing. In contrast, competitors rely on security mechanisms that, at best, represent only a fraction of Ethereum's total security. This gives 0G a distinct advantage, as it leverages the vast economic incentives and decentralization of Ethereum's staking system, providing a level of protection that competitors cannot match. Even more issues exist, including EigenDA's lack of randomization over its data committees. As data committees are core to a DA system's integrity, a lack of randomization means that collusion is theoretically possible for malicious nodes to predict when they might be on a committee together. 0G's core differentiation is massive throughput and scalability. This is possible through 0G's unique design includes a built-in storage system and horizontally scalable consensus design, alongside other clever design mechanisms that we'll cover below. The result is that 0G serves as the foundational layer for decentralized AI applications, bringing on-chain AI and more to life."
      },
      {
        "title": "Why 0G‚Äã",
        "content": "There are 4 differentiators of 0G worth highlighting: 1. Infinitely Scalable DA‚Äã 0G's infinitely scalable DAL can quickly query or confirm data as valid, whether data is held by 0G Storage, or external Web2 or Web3 databases. Infinite scalability comes from the ability to continuously add new consensus networks, supporting workloads that far surpass the capacity of existing systems. 2. Modular and Layered Architecture‚Äã 0G's design decouples storage, data availability, and consensus, allowing each component to be optimized for its specific function. Data availability is ensured through redundancy, with data distributed across decentralized Storage Nodes. Cryptographic proofs (like Merkle trees and zk-proofs) verify data integrity at regular intervals, automatically replacing nodes that fail to produce valid proofs. And combined with 0G's ability to keep adding new consensus networks that scale with demand, 0G can scale efficiently and is ideal for complex AI workflows and large-scale data processing. 3. Decentralized AI Operating System & High Throughput‚Äã 0G is the first decentralized AI operating system (deAIOS) designed to give users control over their data, while providing the infrastructure necessary to handle the massive throughput demands of AI applications. Beyond its modular architecture and infinite consensus layers, 0G achieves high throughput through parallel data processing, enabled by erasure coding, horizontally scalable consensus networks, and more. With a demonstrated throughput of 50 Gbps on the Galileo Testnet, 0G seamlessly supports AI workloads and other high-performance needs, including training large language models and managing AI agent networks. These differentiators make 0G uniquely positioned to tackle the challenges of scaling AI on a decentralized platform, which is critical for the future of Web3 and decentralized intelligence."
      },
      {
        "title": "How Does This Work?‚Äã",
        "content": "As covered in 0G Storage, data within the 0G ecosystem is first erasure-coded and split into \"data chunks,\" which are then distributed across various Storage Nodes in the 0G Storage network. To ensure data availability, 0G uses Data Availability Nodes that are randomly chosen using a Verifiable Random Function (VRF). A VRF generates random values in a way that is unpredictable yet verifiable by others, which is important as it prevents potentially malicious nodes from collusion. These DA nodes work together in small groups, called quorums, to check and verify the stored data. The system assumes that most nodes in each group will act honestly, known as an \"honest majority\" assumption. The consensus mechanism used by 0G is fast and efficient due to its sampling-based approach. Rather than verifying all data, DA nodes sample portions of it, drastically reducing the data they need to handle. Once enough nodes agree that the sampled data is available and correct, they submit availability proofs to the 0G Consensus network. This lightweight, sample-driven approach enables faster verification while maintaining strong security. Validators in the 0G Consensus network verify and finalize DA proofs Validators in the 0G Consensus network, who are separate from the DA nodes, verify and finalize these proofs. Although DA nodes ensure data availability, they do not directly participate in the final consensus process, which is the responsibility of 0G validators. Validators use a shared staking mechanism where they stake 0G tokens on a primary network (likely Ethereum). Any slashable event across connected networks leads to slashing on the main network, securing the system's scalability while maintaining robust security. This is a key mechanism that allows for the system to scale infinitely while maintaining data availability. In return, validators engaged in shared staking receive 0G tokens on any network managed, which can then be burnt in return for 0G tokens on the mainnet. The lightweight, sample-driven consensus approach"
      },
      {
        "title": "Use Cases‚Äã",
        "content": "0G DA offers an infinitely scalable and high-performance DA solution for a wide range of applications across Web3, AI, and more. L1s / L2s‚Äã Layer 1 and Layer 2 chains can utilize 0G DA to handle data availability and storage requirements for decentralized AI models, large datasets, and on-chain applications. Existing partners include networks like Polygon, Optimism, Arbitrum, Fuel, Manta Network, and countless more, which leverage 0G's scalable infrastructure to store data more efficiently and support fast retrieval. Decentralized Shared Sequencers‚Äã Decentralized Shared Sequencers help order L2 transactions before final settlement on Ethereum. By integrating 0G DA, shared sequencers can streamline data across multiple networks in a decentralized manner, unlike existing sequencers which are often centralized. This also means fast and secure data transfers between L2s. Bridges‚Äã Cross-chain bridges benefit from 0G DA's scalable storage and data availability features. Networks can store and retrieve state data using 0G DA, making state migration between networks faster and more secure. For example, a network can confirm a user's assets and transfer them securely to another chain using 0G's highly efficient data verification. Rollups-as-a-Service (RaaS)‚Äã 0G DA can serve as a reliable DA solution for RaaS providers like Caldera and AltLayer, enabling seamless configuration and deployment of rollups. With 0G DA's highly scalable infrastructure, RaaS providers can ensure the secure availability of data across multiple rollups without compromising performance. DeFi‚Äã 0G's DA infrastructure is ideally suited for DeFi applications that require fast settlement and high-frequency trading. For example, by storing order book data on 0G, DeFi projects can achieve faster transaction throughput and enhanced scalability across L2s and L3s. On-Chain Gaming‚Äã On-chain gaming platforms rely on cryptographic proofs and metadata related to player assets, actions, and scores. 0G DA's ability to handle large volumes of data securely and efficiently makes it an optimal solution for gaming applications that require reliable data storage and fast retrieval. Data Markets‚Äã Web3 data markets can benefit from 0G DA by storing datasets on-chain. The decentralized storage and retrieval capabilities of 0G enable real-time updates and querying of data, providing a reliable solution for data market platforms. AI & Machine Learning‚Äã 0G DA is particularly focused on supporting decentralized AI, allowing full AI models and vast datasets to be stored and accessed on-chain. This infrastructure is essential for advanced AI applications that demand high data throughput and availability, such as training large language models (LLMs) and managing entire AI agent Networks."
      },
      {
        "title": "Getting Started‚Äã",
        "content": "Ready to integrate 0G DA into your project? Run a DA Node: Node operator guide Integration Guide: Developer documentation Technical Deep Dive: DA architecture details 0G DA: Bringing infinite scalability to decentralized data availability."
      }
    ]
  },
  {
    "menu": "0G Storage: Built for Massive Data",
    "sections": [
      {
        "title": "What is 0G Storage?‚Äã",
        "content": "0G Storage breaks these tradeoffs - a decentralized storage network that's as fast as AWS S3 but built for Web3. Purpose-designed for AI workloads and massive datasets. New to decentralized storage?"
      },
      {
        "title": "Why Choose 0G Storage?‚Äã",
        "content": "üöÄ The Complete Package‚Äã What You Get Why It Matters 95% lower costs than AWS Sustainable for large datasets Instant retrieval No waiting for critical data Structured + unstructured data One solution for all storage needs Universal compatibility Works with any blockchain or Web2 app Proven scale Already handling TB-scale workloads"
      },
      {
        "title": "How It Works‚Äã",
        "content": "0G Storage is a distributed data storage system designed with on-chain elements to incentivize storage nodes to store data on behalf of users. Anyone can run a storage node and receive rewards for maintaining one. Technical Architecture‚Äã 0G Storage uses a two-lane system: üì§ Data Publishing Lane üíæ Data Storage Lane"
      },
      {
        "title": "Storage Layers for Different Needs‚Äã",
        "content": "üìÅ Log Layer (Immutable Storage)‚Äã Perfect for: AI training data, archives, backups Append-only (write once, read many) Optimized for large files Lower cost for permanent storage Use cases: ML datasets Video/image archives Blockchain history General Large file storage üîë Key-Value Layer (Mutable Storage)‚Äã Perfect for: Databases, dynamic content, state storage Update existing data Fast key-based retrieval Real-time applications Use cases: On-chain databases User profiles Game state Collaborative documents"
      },
      {
        "title": "How Storage Providers Earn‚Äã",
        "content": "0G Storage is maintained by a network of miners incentivized to store and manage data through a unique consensus mechanism known as Proof of Random Access (PoRA). How It Works‚Äã Random Challenges: System randomly asks miners to prove they have specific data Cryptographic Proof: Miners must generate a valid hash (like Bitcoin mining) Quick Response: Must respond fast to prove data is readily accessible Fair Rewards: Successful proofs earn storage fees What's PoRA in simple terms? Fair Competition = Fair Reward‚Äã To promote fairness, the mining range is capped at 8 TB of data per mining operation. Why 8TB limit? Small miners can compete with large operations Prevents centralization Lower barrier to entry For large operators: Run multiple 8TB instances. For individuals: Focus on single 8TB range, still profitable"
      },
      {
        "title": "How 0G Compares‚Äã",
        "content": "Solution Best For Limitation 0G Storage AI/Web3 apps needing speed + scale Newer ecosystem AWS S3 Traditional apps Centralized, expensive Filecoin Cold storage archival Slow retrieval, unstructured only Arweave Permanent storage Extremely expensive IPFS Small files, hobby projects Very slow, no guarantees 0G's Unique Position‚Äã Only solution supporting both structured and unstructured data Instant access unlike other decentralized options Built for AI from the ground up"
      },
      {
        "title": "Frequently Asked Questions‚Äã",
        "content": "Is my data really safe if nodes go offline? How fast can I retrieve large files? What happens to pricing as the network grows? Can I migrate from existing storage?"
      },
      {
        "title": "Get Started‚Äã",
        "content": "üßë‚Äçüíª For Developers‚Äã Integrate 0G Storage in minutes ‚Üí SDK Documentation ‚õèÔ∏è For Storage Providers‚Äã Earn by providing storage capacity ‚Üí Run a Storage Node 0G Storage: Purpose-built for AI and Web3's massive data needs."
      }
    ]
  },
  {
    "menu": "Developer Hub",
    "sections": [
      {
        "title": "0G Services‚Äã",
        "content": "‚õìÔ∏è 0G Chain‚Äã EVM-compatible blockchain optimized for AI Deploy Contracts Precompiles Reference Chain Architecture 0G Compute‚Äã Decentralized GPU marketplace for AI workloads Overview & Architecture SDK Reference CLI Usage Become a Provider üíæ 0G Storage‚Äã High-performance storage for massive datasets SDK Integration CLI Commands Architecture Details üìä 0G DA‚Äã Scalable data availability for any chain Technical Deep Dive Integration Guide Rollup Integrations"
      },
      {
        "title": "Community Projects‚Äã",
        "content": "Explore our growing ecosystem of DeAI applications in the awesome-0g repository, which showcases community projects, tools, and resources built on 0G. Ready to build? Pick a service above and start in minutes, or join our Discord for help."
      }
    ]
  },
  {
    "menu": "0G Testnet (Galileo)",
    "sections": [
      {
        "title": "Network Details‚Äã",
        "content": "Parameters Network Details Network Name 0G-Galileo-Testnet Chain ID 16601 Token Symbol OG RPC Endpoint https://evmrpc-testnet.0g.ai Block Explorer https://chainscan-galileo.0g.ai Faucet https://faucet.0g.ai ‚úÖ 3rd Party RPCs (Recommended for production)‚Äã QuickNode ThirdWeb Ankr"
      },
      {
        "title": "Getting Started‚Äã",
        "content": "Step 1: Add Network to Wallet‚Äã Remove any old 0G testnet configurations before adding Galileo.Need help? Add to MetaMask Add to OKX Wallet .wallet-buttons { display: flex; gap: 16px; margin: 16px 0; } @media (max-width: 768px) { .wallet-buttons { flex-direction: column; } } Step 2: Get Test Tokens‚Äã Visit the 0G Faucet to receive free testnet tokens. Daily Limit: 0.1 OG per wallet. Step 3: Start Building‚Äã Choose your integration: Deploy Smart Contracts Use Storage SDK Access Compute Network Integrate DA Layer Contract Addresses‚Äã CAUTION Addresses may change during testnet. 0G Storage Flow: 0xbD75117F80b4E22698D0Cd7612d92BDb8eaff628 Mine: 0x3A0d1d67497Ad770d6f72e7f4B8F0BAbaa2A649C Market: 0x53191725d260221bBa307D8EeD6e2Be8DD265e19 Reward: 0xd3D4D91125D76112AE256327410Dd0414Ee08Cb4 0G DA DAEntrance: 0xE75A073dA5bb7b0eC622170Fd268f35E675a957B Deployment Block: 326165"
      },
      {
        "title": "Developer Tools‚Äã",
        "content": "Block Explorers‚Äã Chain Explorer: View transactions, blocks, and smart contracts Storage Explorer: Track storage operations and metrics Validator Dashboard: Monitor network validators"
      },
      {
        "title": "Faucet‚Äã",
        "content": "Use the official Faucet to request tokens. Each user can receive up to 0.1 OG token per day, which is sufficient for most testing needs. If you require more than 0.1 OG token per day, please reach out in our vibrant discord community to request additional tokens."
      }
    ]
  },
  {
    "menu": "Building on 0G",
    "sections": [
      {
        "title": "Prerequisites‚Äã",
        "content": "Before building: Get testnet tokens from the faucet Install relevant SDK for your language Review service documentation for your chosen components"
      },
      {
        "title": "What's Next?‚Äã",
        "content": "Based on your needs, dive into: Dapp Migration? ‚Üí Deploy on 0G Chain Need Storage? ‚Üí Storage SDK Guide Need Compute? ‚Üí Compute Integration Building a Rollup? ‚Üí DA Integration Creating INFTs? ‚Üí INFT Overview üí° Pro Tip: Start with one service, prove the value, then expand. Most successful projects begin with 0G Storage or Compute before exploring other services."
      }
    ]
  },
  {
    "menu": "Staking Interfaces",
    "sections": [
      {
        "title": "Overview‚Äã",
        "content": "The 0G Chain staking system enables OG token holders to participate in network consensus and earn rewards through two primary mechanisms: Becoming a Validator: Run infrastructure to validate transactions and produce blocks Delegating to Validators: Stake tokens with existing validators to earn rewards without running infrastructure The staking system is built on two core smart contract interfaces: IStakingContract: Central registry managing validators and global staking parameters IValidatorContract: Individual validator operations including delegations and reward distribution"
      },
      {
        "title": "Prerequisites‚Äã",
        "content": "Before working with the staking interfaces: Familiarity with Solidity and smart contract development Basic knowledge of consensus mechanisms and staking concepts"
      },
      {
        "title": "Quick Start‚Äã",
        "content": "// Create a validator IStakingContract staking = IStakingContract(0xea224dBB52F57752044c0C86aD50930091F561B9); address validator = staking.createAndInitializeValidatorIfNecessary{value: msg.value}( description, commissionRate, withdrawalFee, pubkey, signature ); // Delegate to validator IValidatorContract(validator).delegate{value: msg.value}(msg.sender);"
      },
      {
        "title": "Core Concepts‚Äã",
        "content": "Validators‚Äã Validators process transactions and produce blocks: Unique Identity: Identified by 48-byte consensus public key Operator Control: Managed by an Ethereum address Commission: Set their own reward commission rates Self-Delegation: Required minimum stake from operator Delegations‚Äã Token holders earn rewards by delegating to validators: Share-Based: Delegations represented as shares in validator pool Proportional Rewards: Earnings based on share percentage Withdrawal Delay: Undelegation subject to network delay period Reward Distribution‚Äã Rewards flow through multiple layers: Community Tax: Applied to all rewards first Validator Commission: Taken from remaining rewards Delegator Distribution: Proportional to shares held"
      },
      {
        "title": "Contract Interfaces‚Äã",
        "content": "IStakingContract‚Äã 0xea224dBB52F57752044c0C86aD50930091F561B9 (Testnet) Central registry for validators and global parameters. Validator Management‚Äã // Create validator contract function createValidator(bytes calldata pubkey) external returns (address); // Initialize validator with self-delegation function initializeValidator( Description calldata description, uint32 commissionRate, uint96 withdrawalFeeInGwei, bytes calldata pubkey, bytes calldata signature ) external payable; // Create and initialize in one call function createAndInitializeValidatorIfNecessary( Description calldata description, uint32 commissionRate, uint96 withdrawalFeeInGwei, bytes calldata pubkey, bytes calldata signature ) external payable; Query Functions‚Äã function getValidator(bytes memory pubkey) external view returns (address); function computeValidatorAddress(bytes calldata pubkey) external view returns (address); function validatorCount() external view returns (uint32); function maxValidatorCount() external view returns (uint32); IValidatorContract‚Äã Individual validator operations and delegation management. Delegation Management‚Äã // Delegate tokens (msg.value = amount) function delegate(address delegatorAddress) external payable returns (uint); // Undelegate shares (msg.value = withdrawal fee) function undelegate(address withdrawalAddress, uint shares) external payable returns (uint); // Withdraw validator commission (only validator operator) function withdrawCommission(address withdrawalAddress) external returns (uint); ACCESS CONTROL The withdrawCommission function is restricted to the validator operator only - the address that originally created and manages the validator. Information Queries‚Äã function tokens() external view returns (uint); // Total tokens (delegated + rewards) function delegatorShares() external view returns (uint); // Total shares issued function getDelegation(address delegator) external view returns (address, uint); function commissionRate() external view returns (uint32); function withdrawalFeeInGwei() external view returns (uint96); UNDERSTANDING TOKENS() The tokens() function returns the complete validator balance, including both the original delegated amounts and any accumulated rewards that haven't been distributed yet."
      },
      {
        "title": "Examples‚Äã",
        "content": "Creating a Validator‚Äã // SPDX-License-Identifier: MIT pragma solidity ^0.8.0; import \"./IStakingContract.sol\"; contract ValidatorExample { IStakingContract constant STAKING = IStakingContract(0xea224dBB52F57752044c0C86aD50930091F561B9); function createValidator( bytes calldata pubkey, bytes calldata signature ) external payable { Description memory desc = Description({ moniker: \"My Validator\", identity: \"keybase-id\", website: \"https://validator.example.com\", securityContact: \"security@example.com\", details: \"A reliable 0G Chain validator\" }); STAKING.createAndInitializeValidatorIfNecessary{value: msg.value}( desc, 50000, // 5% commission 1, // 1 Gwei withdrawal fee pubkey, signature ); } } Delegation Management‚Äã contract DelegationHelper { IStakingContract constant STAKING = IStakingContract(0xea224dBB52F57752044c0C86aD50930091F561B9); function delegateToValidator(bytes calldata pubkey) external payable { address validator = STAKING.getValidator(pubkey); require(validator != address(0), \"Validator not found\"); IValidatorContract(validator).delegate{value: msg.value}(msg.sender); } function getDelegationInfo( bytes calldata pubkey, address delegator ) external view returns (uint shares, uint estimatedTokens) { address validator = STAKING.getValidator(pubkey); IValidatorContract v = IValidatorContract(validator); (, shares) = v.getDelegation(delegator); uint totalTokens = v.tokens(); uint totalShares = v.delegatorShares(); if (totalShares > 0) { estimatedTokens = (shares * totalTokens) / totalShares; } } }"
      },
      {
        "title": "Getting Validator Signature‚Äã",
        "content": "Prerequisites‚Äã Your directory structure should look like: galileo/ ‚îú‚îÄ‚îÄ bin/0gchaind ‚îî‚îÄ‚îÄ config/ ‚îú‚îÄ‚îÄ genesis.json ‚îú‚îÄ‚îÄ priv_validator_key.json ‚îî‚îÄ‚îÄ ... Step 1: Extract Public Key‚Äã # Set your home directory HOMEDIR=/.tmp/0gchaind CHAIN_SPEC=devnet # Generate validator keys ./bin/0gchaind deposit validator-keys --home $HOMEDIR --chaincfg.chain-spec=$CHAIN_SPEC Output: Eth/Beacon Pubkey (Compressed 48-byte Hex): 0xaa0f99735a6436d6b7ed763c2eaa8452d753c5152a4fb1e4dc0bd7e33bcfc8cd4fac0e2d6cbab941f423c17728fecc56 Step 2: Compute Validator Address‚Äã Use the public key from Step 1 to compute the validator's contract address. Choose your preferred method: Foundry Cast curl/RPC Recommended method - simpler syntax and better error handling. # Set your params STAKING_CONTRACT_ADDRESS=0xea224dBB52F57752044c0C86aD50930091F561B9 PUBLIC_KEY=0xaa0f99735a6436d6b7ed763c2eaa8452d753c5152a4fb1e4dc0bd7e33bcfc8cd4fac0e2d6cbab941f423c17728fecc56 # cast call cast call $STAKING_CONTRACT_ADDRESS \"computeValidatorAddress(bytes)(address)\" $PUBLIC_KEY --rpc-url https://evmrpc-testnet.0g.ai Output: 0x1e776a6b65892ec60537a885c17b820301e054b9 Step 3: Generate Signature‚Äã Use the validator's contract address from Step 2 to generate signature. # Set your home directory HOMEDIR=/.tmp/0gchaind CHAIN_SPEC=devnet VALIDATOR_CONTRACT_ADDRESS=0x1e776a6b65892ec60537a885c17b820301e054b9 VALIDATOR_INITIAL_DELEGATION_IN_GWEI=32000000000 # 32 ethers # Generate signature for validator initialization .bin/0gchaind deposit create-validator $VALIDATOR_CONTRACT_ADDRESS $VALIDATOR_INITIAL_DELEGATION_IN_GWEI $HOMEDIR/config/genesis.json --home $HOMEDIR --chaincfg.chain-spec=$CHAIN_SPEC Output: ‚úÖ Deposit message created successfully! pubkey: 0xaa0f99735a6436d6b7ed763c2eaa8452d753c5152a4fb1e4dc0bd7e33bcfc8cd4fac0e2d6cbab941f423c17728fecc56 signature: 0x123456789000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"
      },
      {
        "title": "Data Structures‚Äã",
        "content": "Description Struct Withdrawal Entry"
      },
      {
        "title": "Configuration Parameters‚Äã",
        "content": "Parameter Description maxValidatorCount Maximum validators allowed minActivationStakesInGwei Minimum stake for activation maxEffectiveStakesInGwei Maximum effective stake communityTaxRate Tax on all rewards minWithdrawabilityDelay Withdrawal delay blocks"
      },
      {
        "title": "Troubleshooting‚Äã",
        "content": "Error: \"Validator not found\" Error: \"DelegationBelowMinimum\" Error: \"NotEnoughWithdrawalFee"
      },
      {
        "title": "Contract Addresses‚Äã",
        "content": "Network Staking Contract Testnet 0xea224dBB52F57752044c0C86aD50930091F561B9"
      },
      {
        "title": "Resources‚Äã",
        "content": "Run Validator Node: Validator Setup Guide GitHub Repository: 0G Chain Contracts Deploy Contracts: Contract Deployment Need help? Join our Discord for developer support."
      }
    ]
  },
  {
    "menu": "0G Chain Precompiles",
    "sections": [
      {
        "title": "What Are Precompiles?‚Äã",
        "content": "Precompiles are special contracts deployed at fixed addresses that execute native code instead of EVM bytecode. They provide: Gas Efficiency: 10-100x cheaper than Solidity implementations Native Features: Access chain-level functionality Complex Operations: Cryptographic functions and state management"
      },
      {
        "title": "0G Chain Precompiles‚Äã",
        "content": "Beyond standard Ethereum precompiles, 0G Chain adds specialized contracts for decentralized AI infrastructure: üîê DASigners‚Äã 0x0000000000000000000000000000000000001000 Manages data availability signatures for 0G's DA layer. Key Features: Register and manage DA node signers Query quorum information Verify data availability proofs Common Use Case: Building applications that need to verify data availability directly on-chain. ü™ô WrappedOGBase‚Äã 0x0000000000000000000000000000000000001002 Wrapped version of native OG token for DeFi compatibility. Key Features: Wrap/unwrap native OG tokens ERC20-compatible interface Efficient gas operations Common Use Case: Integrating OG tokens with DEXs, lending protocols, or other DeFi applications. Questions? Get help in our Discord #dev-support channel."
      }
    ]
  },
  {
    "menu": "Overview",
    "sections": [
      {
        "title": "Structs‚Äã",
        "content": "SignerDetail‚Äã struct SignerDetail { address signer; string socket; BN254.G1Point pkG1; BN254.G2Point pkG2; } Description: Contains details of a signer, including the address, socket, and bn254 public keys (G1 and G2 points). Fields: signer: The address of the signer. socket: The socket associated with the signer. pkG1: The G1 public key of the signer. pkG2: The G2 public key of the signer. Params‚Äã struct Params { uint tokensPerVote; uint maxVotesPerSigner; uint maxQuorums; uint epochBlocks; uint encodedSlices; } Description: Defines parameters for the DAsigners module. Fields: tokensPerVote: The number of tokens required for one vote. maxVotesPerSigner: The maximum number of votes a signer can cast. maxQuorums: The maximum number of quorums allowed. epochBlocks: The number of blocks in an epoch. encodedSlices: The number of encoded slices in one DA blob."
      },
      {
        "title": "Functions‚Äã",
        "content": "params()‚Äã function params() external view returns (Params memory); Description: Retrieves the current parameters of the DAsigners module. Returns: Params structure containing the current module parameters. epochNumber()‚Äã function epochNumber() external view returns (uint); Description: Returns the current epoch number. Returns: uint representing the current epoch number. quorumCount(uint _epoch)‚Äã function quorumCount(uint _epoch) external view returns (uint); Description: Returns the number of quorums for a given epoch. Parameters: _epoch: The epoch number. Returns: uint representing the quorum count for the given epoch. isSigner(address _account)‚Äã function isSigner(address _account) external view returns (bool); Description: Checks if a given account is a registered signer. Parameters: _account: The address to check. Returns: bool indicating whether the account is a signer. getSigner(address[] memory _account)‚Äã function getSigner( address[] memory _account ) external view returns (SignerDetail[] memory); Description: Retrieves details for the signers of the provided addresses. Parameters: _account: An array of addresses to fetch the signer details for. Returns: An array of SignerDetail structures for each signer. getQuorum(uint _epoch, uint _quorumId)‚Äã function getQuorum( uint _epoch, uint _quorumId ) external view returns (address[] memory); Description: Returns the addresses of the members in a specific quorum for a given epoch. Parameters: _epoch: The epoch number. _quorumId: The ID of the quorum. Returns: An array of addresses that are members of the quorum. getQuorumRow(uint _epoch, uint _quorumId, uint32 _rowIndex)‚Äã function getQuorumRow( uint _epoch, uint _quorumId, uint32 _rowIndex ) external view returns (address); Description: Retrieves a specific address from a quorum's row for a given epoch and quorum ID. Parameters: _epoch: The epoch number. _quorumId: The quorum ID. _rowIndex: The row index within the quorum. Returns: The address at the specified row index in the quorum. registerSigner(SignerDetail memory _signer, BN254.G1Point memory _signature)‚Äã function registerSigner( SignerDetail memory _signer, BN254.G1Point memory _signature ) external; Description: Registers a new signer with the provided details and signature. Parameters: _signer: The details of the signer to register. _signature: The signature to verify the registration. updateSocket(string memory _socket)‚Äã function updateSocket(string memory _socket) external; Description: Updates the socket used by the module. Parameters: _socket: The new socket address to update. registeredEpoch(address _account, uint _epoch)‚Äã function registeredEpoch( address _account, uint _epoch ) external view returns (bool); Description: Checks if a specific account is registered in a given epoch. Parameters: _account: The address to check. _epoch: The epoch number. Returns: bool indicating whether the account is registered for the specified epoch. registerNextEpoch(BN254.G1Point memory _signature)‚Äã function registerNextEpoch(BN254.G1Point memory _signature) external; Description: Registers the next epoch using the provided signature. Parameters: _signature: The signature used to register the next epoch. getAggPkG1(uint _epoch, uint _quorumId, bytes memory _quorumBitmap)‚Äã function getAggPkG1( uint _epoch, uint _quorumId, bytes memory _quorumBitmap ) external view returns (BN254.G1Point memory aggPkG1, uint total, uint hit); Description: Retrieves the aggregated public key for a given epoch and quorum ID. Parameters: _epoch: The epoch number. _quorumId: The quorum ID. _quorumBitmap: The quorum bitmap. Returns: aggPkG1: The aggregated public key. total: The number of rows. hit: The number of rows that contributed to the aggregation."
      }
    ]
  },
  {
    "menu": "Overview",
    "sections": [
      {
        "title": "Structs‚Äã",
        "content": "Supply‚Äã struct Supply { uint256 cap; uint256 initialSupply; uint256 supply; } Description: Defines the supply details of a minter, including the cap, initial supply, and the current supply. Fields: cap: The maximum allowed mint supply for the minter. initialSupply: The initial mint supply to the minter, equivalent to the initial allowed burn amount. supply: The current mint supply used by the minter, set to initialSupply at beginning."
      },
      {
        "title": "Functions‚Äã",
        "content": "getWA0GI()‚Äã function getWA0GI() external view returns (address); Description: Retrieves the address of the wrapped OG (WA0GI) contract. Returns: address of the WOG contract. minterSupply(address minter)‚Äã function minterSupply(address minter) external view returns (Supply memory); Description: Retrieves the mint supply details for a given minter. Parameters: minter: The address of the minter. Returns: A Supply structure containing the mint cap, initial supply, and current supply of the specified minter. mint(address minter, uint256 amount)‚Äã function mint(address minter, uint256 amount) external; Description: Mints OG to WA0GI contract and adds the corresponding amount to the minter's mint supply. If the minter's final mint supply exceeds their mint cap, the transaction will revert. Parameters: minter: The address of the minter. amount: The amount of OG to mint. Restrictions: Can only be called by the WA0GI contract. burn(address minter, uint256 amount)‚Äã function burn(address minter, uint256 amount) external; Description: Burns the specified amount of OG in WA0GI contract on behalf of the minter and reduces the corresponding amount from the minter's mint supply. Parameters: minter: The address of the minter. amount: The amount of OG to burn. Restrictions: Can only be called by the WOG contract."
      }
    ]
  },
  {
    "menu": "0G Compute Network",
    "sections": [
      {
        "title": "AI Computing Costs Are Crushing Innovation‚Äã",
        "content": "Running AI models today means choosing between: Cloud Providers: $5,000-50,000/month for dedicated GPUs API Services: $0.03+ per request, adding up to thousands monthly Building Infrastructure: Millions in hardware investment Result: Only well-funded companies can afford AI at scale."
      },
      {
        "title": "Decentralized GPU Marketplace‚Äã",
        "content": "0G Compute Network connects idle GPU owners with AI developers, creating a marketplace that's: 90% Cheaper: Pay only for compute used, no monthly minimums Instantly Available: Access 1000s of GPUs globally Verifiable: Cryptographic proofs ensure computation integrity Think of it as \"Uber for GPUs\" - matching supply with demand efficiently."
      },
      {
        "title": "Architecture Overview‚Äã",
        "content": "The network consists of: Smart Contracts: Handle payments and verification Provider Network: GPU owners running compute services Client SDKs: Easy integration for developers Verification Layer: Ensures computation integrity"
      },
      {
        "title": "Key Components‚Äã",
        "content": "ü§ñ Supported Services‚Äã Service Type What It Does Status Inference Run pre-trained models (LLMs) ‚úÖ Live Fine-tuning Fine-tune models with your data ‚úÖ Live Training Train models from scratch üîú Coming üîê Trust & Verification‚Äã Verifiable Computation: Proof that work was done correctly TEE (Trusted Execution Environment) for secure processing Cryptographic signatures on all results Can't fake or manipulate outputs What makes it trustworthy?"
      },
      {
        "title": "Quick Start Paths‚Äã",
        "content": "üë®‚Äçüíª \"I want to use AI services\"‚Äã Build AI-powered applications without infrastructure: Install SDK - 5 minute setup Fund your account - Pre-pay for usage Send requests - OpenAI SDK compatible üñ•Ô∏è \"I have GPUs to monetize\"‚Äã Turn idle hardware into revenue: Check hardware requirements Set up provider software üéØ \"I need to fine-tune AI models\"‚Äã Fine-tune models with your data: Install CLI tools Prepare your dataset Start fine-tuning"
      },
      {
        "title": "Frequently Asked Questions‚Äã",
        "content": "How much can I save compared to OpenAI? Is my data secure? How fast is it compared to centralized services? 0G Compute Network: Democratizing AI computing for everyone."
      }
    ]
  },
  {
    "menu": "0G Compute SDK",
    "sections": [
      {
        "title": "Quick Start‚Äã",
        "content": "Installation‚Äã pnpm add @0glabs/0g-serving-broker @types/crypto-js@4.2.2 crypto-js@4.2.0"
      },
      {
        "title": "Core Concepts‚Äã",
        "content": "1. The Broker‚Äã Your interface to the 0G Compute Network: Handles authentication and billing Manages provider connections Verifies computations 2. Providers‚Äã GPU owners offering AI services: Each has a unique address Set their own prices Run specific models 3. Prepaid Accounts‚Äã Fund account before usage Automatic micropayments No surprise bills"
      },
      {
        "title": "Step-by-Step Guide‚Äã",
        "content": "Initialize the Broker‚Äã Using Private Key Browser Wallet import { ethers } from \"ethers\"; import { createZGComputeNetworkBroker } from \"@0glabs/0g-serving-broker\"; const provider = new ethers.JsonRpcProvider(\"https://evmrpc-testnet.0g.ai\"); const wallet = new ethers.Wallet(process.env.PRIVATE_KEY!, provider); const broker = await createZGComputeNetworkBroker(wallet); Fund Your Account‚Äã // Add 0.1 OG tokens (~10,000 requests) await broker.ledger.addLedger(ethers.parseEther(\"0.1\")); // Check balance const account = await broker.ledger.getLedger(); console.log(`Balance: ${ethers.formatEther(account.balance)} OG`); Discover Available Services‚Äã The 0G Compute Network hosts multiple AI service providers. The service discovery process helps you find and select the appropriate services for your needs. üéØ Official 0G Services Model Provider Address Description Verification llama-3.3-70b-instruct 0xf07240Efa67755B5311bc75784a061eDB47165Dd State-of-the-art 70B parameter model for general AI tasks TEE (TeeML) deepseek-r1-70b 0x3feE5a4dd5FDb8a32dDA97Bed899830605dBD9D3 Advanced reasoning model optimized for complex problem solving TEE (TeeML) const services = await broker.inference.listService(); Each service contains the following information: type ServiceStructOutput = { provider: string; // Provider's wallet address (unique identifier) serviceType: string; // Type of service url: string; // Service URL inputPrice: bigint; // Price for input processing outputPrice: bigint; // Price for output generation updatedAt: bigint; // Last update timestamp model: string; // Model identifier verifiability: string; // Indicates how the service's outputs can be verified. 'TeeML' means it runs with verification in a Trusted Execution Environment. An empty value means no verification. }; Acknowledge Provider‚Äã Before using a service provided by a provider, you must first acknowledge the provider on-chain by following API: await broker.inference.acknowledgeProviderSigner(providerAddress) The providerAddress can be obtained from from service metadata. For details on how to retrieve it, see Discover Available Services Service Requests‚Äã Service usage in the 0G Network involves two key steps: Retrieving service metadata Generating authenticated request headers // Get service details const { endpoint, model } = await broker.inference.getServiceMetadata(provider); // Generate auth headers (single use) const headers = await broker.inference.getRequestHeaders(provider, question); Send a Request to the Service‚Äã Using Fetch API Using OpenAI SDK const response = await fetch(`${endpoint}/chat/completions`, { method: \"POST\", headers: { \"Content-Type\": \"application/json\", ...headers }, body: JSON.stringify({ messages: [{ role: \"user\", content: question }], model: model, }), }); const data = await response.json(); const answer = data.choices[0].message.content; Response Processing‚Äã This function is used to verify the response. If it is a verifiable service, it will return whether the response is valid. const valid = await broker.inference.processResponse( providerAddress, content, chatID // Optional: Only for verifiable services ); Fee Settlement‚Äã Fee settlement by the broker service occurs at scheduled intervals."
      },
      {
        "title": "Account Management‚Äã",
        "content": "Check Balance‚Äã const ledger = await broker.ledger.getLedger(); console.log(` Balance: ${ethers.formatEther(ledger.balance)} OG Locked: ${ethers.formatEther(ledger.locked)} OG Available: ${ethers.formatEther(ledger.balance - ledger.locked)} OG `); Add Funds‚Äã // Add more funds await broker.ledger.depositFund(ethers.parseEther(\"0.5\")); Request Refund‚Äã // Withdraw unused funds const amount = ethers.parseEther(\"0.1\"); await broker.ledger.retrieveFund(\"inference\", amount);"
      },
      {
        "title": "Troubleshooting‚Äã",
        "content": "Common Issues‚Äã Error: Insufficient balance Error: Headers already used Error: Provider not responding"
      },
      {
        "title": "Next Steps‚Äã",
        "content": "Fine-tune Models ‚Üí CLI Guide Become a Provider ‚Üí Provider Setup View Examples ‚Üí GitHub Questions? Join our Discord for support."
      }
    ]
  },
  {
    "menu": "Fine-tuning CLI",
    "sections": [
      {
        "title": "Quick Start‚Äã",
        "content": "Prerequisites‚Äã Node version >= 22.0.0 Install CLI‚Äã pnpm install @0glabs/0g-serving-broker -g Set Environment‚Äã export RPC_ENDPOINT=https://evmrpc-testnet.0g.ai # Optional, this is default export ZG_PRIVATE_KEY=your_private_key_here Create Account & Add Funds‚Äã The Fine-tuning CLI requires an account to pay for service fees via the 0G Compute Network. You can create an account with the following command: # Create account with 0.1 OG 0g-compute-cli add-account --amount 0.1 List Providers‚Äã 0g-compute-cli list-providers The output will be like: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Provider 1 ‚îÇ 0xf07240Efa67755B5311bc75784a061eDB47165Dd ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Available ‚îÇ ‚úì ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Price Per Byte in Dataset (OG) ‚îÇ 0.000000000000000001 ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Provider 2 ‚îÇ ...... ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ ...... ‚îÇ ...... ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Provider x: The address of the provider. The address of the official provider is 0xf07240Efa67755B5311bc75784a061eDB47165Dd. Available: Indicates if the provider is available. If ‚úì, the provider is available. If ‚úó, the provider is occupied. Price Per Byte in Dataset (OG): The service fee charged by the provider. The fee is currently based on the byte count of the dataset. Future versions may charge more accurately based on the token count of the dataset. List Models‚Äã # List available models 0g-compute-cli list-models üìã Available Models Summary The output consists of two main sections: Predefined Models: These are models that are provided by the system as predefined options. They are typically built-in, curated, and maintained to ensure quality, reliability, and broad applicability across common use cases. Provider's Model: These models are offered by external service providers. Providers may customize or fine-tune models to address specific needs, industries, or advanced use cases. The availability and quality of these models may vary depending on the provider. Note: We currently offer the models listed above as presets. You can choose one of these models for fine-tuning. More models will be provided in future versions. Prepare Configuration File‚Äã Please download the parameter file template for the model you wish to fine-tune from the releases page and modify it according to your needs. Note: For custom models provided by third-party Providers, you can download the usage template including instructions on how to construct the dataset and training configuration using the following command: 0g-compute-cli model-usage --provider <PROVIDER_ADDRESS> --model <MODEL_NAME> --output <PATH_TO_SAVE_MODEL_USAGE> Prepare Your Data‚Äã Please download the dataset format specification and verification script from the releases page to make sure your generated dataset complies with the requirements. Upload Dataset‚Äã # Upload to 0G Storage 0g-compute-cli upload --data-path <PATH_TO_DATASET> # Output: Root hash: 0xabc123... (save this!) Record the root hash of the dataset; they will be needed in later steps. Calculate Dataset Size‚Äã After uploading the dataset to storage, you can calculate its size by running the following command: 0g-compute-cli calculate-token --model <MODEL_NAME> --dataset-path <PATH_TO_DATASET> --provider <PROVIDER_ADDRESS> Create Task‚Äã After calculating the dataset size, you can create a task by running the following command: 0g-compute-cli create-task --provider <PROVIDER_ADDRESS> --model <MODEL_NAME> --dataset <DATASET_ROOT_HASH> --config-path <PATH_TO_CONFIG_FILE> --data-size <DATASET_SIZE> Parameters: Parameter Description --provider Address of the service provider --model Name of the pretrained model --dataset Root hash of the dataset on 0G Storage --config-path Path to the parameter file --data-size Size of the dataset --gas-price Gas price (optional) The output will be like: Verify provider... Provider verified Creating task... Created Task ID: 6b607314-88b0-4fef-91e7-43227a54de57 Note: When creating a task for the same provider, you must wait for the previous task to be completed (status Finished) before creating a new task. If the provider is currently running other tasks, you will be prompted to choose between adding your task to the waiting queue or canceling the request. Monitor Progress‚Äã You can monitor the progress of your task by running the following command: 0g-compute-cli get-task --provider <PROVIDER_ADDRESS> --task <TASK_ID> The output will be like: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Field ‚îÇ Value ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ ID ‚îÇ beb6f0d8-4660-4c62-988d-00246ce913d2 ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Created At ‚îÇ 2025-03-11T01:20:07.644Z ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Pre-trained Model Hash ‚îÇ 0xcb42b5ca9e998c82dd239ef2d20d22a4ae16b3dc0ce0a855c93b52c7c2bab6dc ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Dataset Hash ‚îÇ 0xaae9b4e031e06f84b20f10ec629f36c57719ea512992a6b7e2baea93f447a5fa ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Training Params ‚îÇ {......} ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Fee (neuron) ‚îÇ 179668154 ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Progress ‚îÇ Delivered ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Field Descriptions: ID: Unique identifier for your fine-tuning task Pre-trained Model Hash: Storage reference for the base model being fine-tuned Dataset Hash: Storage reference for your training dataset Training Params: Configuration parameters used during fine-tuning Fee (neuron): Total cost for the fine-tuning task Progress: Task status. Possible values are Init, SettingUp, SetUp, Training, Trained, Delivering, Delivered, UserAcknowledged, Finished, Failed. These represent the following states, respectively: Init: Task submitted SettingUp: Provider is preparing the environment to run the task SetUp: Provider is ready to start training the model Training: Provider is training the model Trained: provider has finished the training Delivering: Provider is uploading the fine-tuning result to storage Delivered: provider has uploaded the fine-tuning result UserAcknowledged: User has confirmed the result is downloadable Finished: Task is completed Failed: Task failed View Task Logs‚Äã You can view the logs of your task by running the following command: 0g-compute-cli get-log --provider <PROVIDER_ADDRESS> --task <TASK_ID> The output will be like: creating task.... Step: 0, Logs: {'loss': ..., 'accuracy': ...} ... Training model for task beb6f0d8-4660-4c62-988d-00246ce913d2 completed successfully Confirm Task Result‚Äã Use the Check Task command to view task status. When the status changes to Delivered, it indicates that the provider has completed the fine-tuning task and uploaded the result to storage. The corresponding root hash has also been saved to the contract. You can download the model with the following command; CLI will download the model based on the root hash submitted by the provider. If the download is successful, CLI updates the contract information to confirm the model is downloaded. 0g-compute-cli acknowledge-model --provider <PROVIDER_ADDRESS> --data-path <PATH_TO_SAVE_MODEL> Note: The model file downloaded with the above command is encrypted, and additional steps are required for decryption. Decrypt Model‚Äã The provider will check the contract to verify if the user has confirmed the download, enabling the provider to settle fees successfully on the contract subsequently. Once the provider confirms the download, it uploads the key required for decryption to the contract, encrypted with the user's public key, and collects the fee. You can again use the get-task command to view the task status. When the status changes to Finished, it means the provider has uploaded the key. At this point, you can decrypt the model with the following command: 0g-compute-cli decrypt-model --provider <PROVIDER_ADDRESS> --encrypted-model <PATH_TO_ENCRYPTED_MODEL> --output <PATH_TO_SAVE_DECRYPTED_MODEL> The above command performs the following operations: Gets the encrypted key from the contract uploaded by the provider Decrypts the key using the user's private key Decrypts the model with the decrypted key Note: The decrypted result will be saved as a zip file. Ensure that the <PATH_TO_SAVE_DECRYPTED_MODEL> ends with .zip (e.g., model_output.zip). After downloading, unzip the file to access the decrypted model. Account Management‚Äã View Account‚Äã 0g-compute-cli get-account Possible output: Overview ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Balance ‚îÇ Value (OG) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Total ‚îÇ 0.999999999820331942 ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Locked (transferred to sub-accounts) ‚îÇ 0.000000000179668154 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Fine-tuning sub-accounts (Dynamically Created per Used Provider) ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Provider ‚îÇ Balance (OG) ‚îÇ Requested Return to Main Account (OG) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ 0x3C44CdDdB6a900fa2b585dd299e03d12FA4293BC ‚îÇ 0.000000000179668154 ‚îÇ 0.000000000000000000 ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ ...... ‚îÇ ...... ‚îÇ ...... ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Overview: Provides a general overview of the account's balance. Total: The current balance of the account Locked: The cumulative amount locked in all sub-accounts Fine-tuning sub-accounts: Information about sub-accounts, with each sub-account corresponding to a provider for paying the provider's service fee. Each sub-account is dynamically created when tasks are submitted. Provider: Address of the provider corresponding to the sub-account Balance: Balance of the sub-account, which is an amount transferred from the main account to the sub-account based on the task fee whenever a task is created. Requested Return to Main Account: Amount requested to be returned from sub-accounts to the main account. If the amount in the sub-account goes unspent for any reason, such as a task failure, you can use the return-funds command to return the balance to the main account. However, it won't return immediately and will only be available after a lock-in period. For details, refer to Retrieving Funds. Note: For more information about sub-accounts, refer to View Sub-Account. Deposit‚Äã You can deposit into your account using the following command. 0g-compute-cli deposit --amount <AMOUNT> Withdrawal‚Äã You can withdraw to your wallet with the following command: 0g-compute-cli refund --amount <AMOUNT> Note: You can't withdraw the \"Lock\" amount in the account; only the \"Total-Lock\" portion can be withdrawn. View Sub-Account‚Äã Sub-accounts are dynamically created when tasks are submitted and used to pay provider service fees. You can view sub-account information with the following command: 0g-compute-cli get-sub-account --provider <PROVIDER_ADDRESS> Possible output: Overview ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Field ‚îÇ Value ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Provider ‚îÇ 0x3C44CdDdB6a900fa2b585dd299e03d12FA4293BC ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Balance (OG) ‚îÇ 0.000000000179668154 ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Funds Applied for Return to Main Account (OG) ‚îÇ 0.000000000179668154 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Details of Each Amount Applied for Return to Main Account ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Amount (OG) ‚îÇ Remaining Locked Time ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ 0.000000000179668154 ‚îÇ 23h 58min 34s ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Deliverables ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Root Hash ‚îÇ Access Confirmed ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ 0x24951e897b1203e8aa1692736837f089a95b70390cc02723505e41ebf9 ‚îÇ ‚úì ‚îÇ ‚îÇ cac70c ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ 0x85b3869bcf14569bb41c3d7d499c9a8eb441e6d606bbe3e10e0fac90e5 ‚îÇ ‚îÇ ‚îÇ 7d36a4 ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Overview: An overview of the account Provider: Address of the provider corresponding to the sub-account Balance: Balance of the sub-account. The main account transfers a certain amount to the sub-account based on the task fee every time a task is created. Funds Applied for Return to Main Account: Amount in the sub-account requested to be returned to the main account Details of Each Amount Applied for Return to Main Account: Detailed information about amounts requested to be returned to the main account Amount: Amount requested to be returned to the main account Remaining Locked Time: Remaining locked time for the return amount to be available in the main account Deliverables: Deliverables issued by the provider after task completion Root Hash: Root hash of the model uploaded to storage Access Confirmed: Indicates whether the user has confirmed download access to the model based on the root hash Retrieve Funds‚Äã The retrieve funds operation returns the balance from sub-accounts to the main account. This operation is asynchronous and will execute after a specific locking period of 24 hours. The lock time ensures provider rights protection, preventing the user from immediately returning the balance to the main account after provider services are rendered and stopping the provider from getting paid. 0g-compute-cli retrieve-fund The above command requests the balance from all sub-accounts to be returned to the main account. After the lock-in period elapses, execute the retrieve-fund command again to refund all the amounts whose locking period has concluded to the main account. Check the refund status using the View Sub-Account command. Other Commands‚Äã View Task List‚Äã You can view the list of tasks submitted to a specific provider using the following command: 0g-compute-cli list-tasks --provider <PROVIDER_ADDRESS> Download Data‚Äã You can download previously uploaded datasets using the command below: 0g-compute-cli download --data-path <PATH_TO_SAVE_DATASET> --data-root <DATASET_ROOT_HASH> Cancel a Task‚Äã You can cancel a task before it starts running using the following command: 0g-compute-cli cancel-task --provider <PROVIDER_ADDRESS> --task <TASK_ID> Note: Tasks that are already in progress or completed cannot be canceled."
      },
      {
        "title": "Troubleshooting‚Äã",
        "content": "Error: Provider busy Error: Insufficient balance"
      }
    ]
  },
  {
    "menu": "Become an Inference Provider",
    "sections": [
      {
        "title": "Why Become a Provider?‚Äã",
        "content": "Monetize Your Infrastructure: Turn idle GPU resources into revenue Automated Settlements: The broker handles billing and payments automatically Trust Through Verification: Offer verifiable services for premium rates"
      },
      {
        "title": "Prerequisites‚Äã",
        "content": "Docker Compose 1.27+ OpenAI-compatible model service Wallet with OG tokens for gas fees (see Testnet Details)"
      },
      {
        "title": "Setup Process‚Äã",
        "content": "Prepare Your Model Service‚Äã Service Interface Requirements‚Äã Your AI service must implement the OpenAI API Interface for compatibility. This ensures consistent user experience across all providers. Verification Interfaces‚Äã To ensure the integrity and trustworthiness of services, different verification mechanisms are employed. Each mechanism comes with its own specific set of protocols and requirements to ensure service verification and security. TEE Verification (TeeML) OPML, ZKML (Coming Soon) TEE (Trusted Execution Environment) verification ensures your computations are tamper-proof. Services running in TEE: Generate signing keys within the secure environment Provide CPU and GPU attestations Sign all inference results These attestations should include the public key of the signing key, verifying its creation within the TEE. All inference results must be signed with this signing key. Hardware Requirements‚Äã CPU: Intel TDX (Trusted Domain Extensions) enabled GPU: NVIDIA H100 or H200 with TEE support Implementation‚Äã 1. Attestation Download Interface‚Äã Expose this endpoint: GET https://{PUBLIC_IP}/attestation/report Return format: { \"signing_address\": \"0x...\", \"nvidia_payload\": \"...\" } Note: The nvidia_payload must be verifiable via NVIDIA's GPU Attestation API. Support for decentralized TEE attestation is planned for the future, and relevant interfaces will be provided. Stay tuned. 2. Signature Download Interface‚Äã For each response with a unique ID: GET https://{PUBLIC_IP}/signature/{response_id} Requirements: Use ECDSA algorithm for signatures Signatures must be verifiable with the signing address Include both request and response content in signature Download and Configure Inference Broker‚Äã To register and manage services, handle user request proxies, and perform settlements, you need to use the Inference Broker. Please visit the releases page to download and extract the latest version of the installation package. # Download from releases page wget https://github.com/0glabs/0g-serving-broker/releases/download/v0.2.0/inference-broker.tar.gz tar -xzf inference-broker.tar.gz cd inference-broker # Copy configuration template cp config.example.yaml config.local.yaml Edit config.local.yaml: servingUrl: \"https://your-public-ip:8080\" # Public endpoint privateKeys: \"YOUR_WALLET_PRIVATE_KEY\" # For settlements targetUrl: \"http://localhost:8000\" # Your model service model: \"llama-3.3-70b-instruct\" # Model identifier SERVING URL Serving URL must be publically accessible from the internet. Configure Docker Port‚Äã Configure the Docker port to match your servingUrl port from config.local.yaml. Replace #PORT# in the docker-compose.yml file with the same port you specified in your servingUrl. For example, if your servingUrl is \"https://your-public-ip:8080\", use port 8080: # Replace #PORT# with your service port (must match servingUrl) sed -i 's/#PORT#/8080/g' docker-compose.yml PORT CONSISTENCY Ensure the port in docker-compose.yml matches the port in your servingUrl from config.local.yaml. Mismatched ports will prevent the service from being accessible. Launch Provider Broker‚Äã # Start the broker service docker compose -f docker-compose.yml up -d # Monitor logs docker compose logs -f The broker will: Register your service on the network Handle user authentication and request routing Manage automatic settlement of payments"
      },
      {
        "title": "Troubleshooting‚Äã",
        "content": "Broker fails to start Service not accessible Settlement issues"
      },
      {
        "title": "Next Steps‚Äã",
        "content": "Join Community ‚Üí Discord for support Explore SDK ‚Üí SDK Documentation for integration details"
      }
    ]
  },
  {
    "menu": "Become a Fine-tuning Provider",
    "sections": [
      {
        "title": "Prerequisites‚Äã",
        "content": "Docker and Docker Compose TDX-enabled Intel CPU Compatible NVIDIA GPU (H100/H200 with TEE support) Wallet with 0G tokens for gas fees Publicly accessible server"
      },
      {
        "title": "Preparation‚Äã",
        "content": "Download the Installation Package‚Äã Visit the Releases Page: 0G Serving Broker Releases Download and Extract: Get the latest version of the fine-tuning installation package. Configuration Setup‚Äã Copy the Config File: Duplicate config.example.yaml to create config.local.yaml. cp config.example.yaml config.local.yaml Modify Settings: Set servingUrl to your publicly accessible URL. Set privateKeys using your wallet's private key for the 0G blockchain. Edit docker-compose.yml: Replace #PORT# with the desired port, matching the port in config.local.yaml. # Replace #PORT# with your service port sed -i 's/#PORT#/8080/g' docker-compose.yml Supporting Custom Models from Providers‚Äã To include custom models, refer to the example configuration below and update your config.local.yaml file accordingly. Ensure that all required fields are properly defined to match your specific model setup. service: customizedModels: - name: \"deepseek-r1-distill-qwen-1.5b\" hash: \"<MODEL_ROOT_HASH>\" image: \"deepseek:latest\" dataType: \"text\" trainingScript: \"/app/finetune.py\" description: \"DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\" tokenizer: \"<TOKENIZER_ROOT_HASH>\" usageFile: \"<ZIP_FILE>\" - name: \"mobilenet_v2\" hash: \"<MODEL_ROOT_HASH>\" image: \"mobilenetV2:latest\" dataType: \"image\" trainingScript: \"/app/finetune.py\" description: \"MobileNet V2 model pre-trained on ImageNet-1k at resolution 224x224.\" tokenizer: \"<TOKENIZER_ROOT_HASH>\" usageFile: \"<ZIP_FILE>\" Configuration Fields: name: Model identifier hash: The root hash of the pre-trained model, obtained after uploading the model to 0G storage. image: The Docker image that encapsulates the fine-tuning execution environment. dataType: Specifies the type of dataset the model is intended to train on. Valid options include text or image. trainingScript: Specifies the path to the training script within the container. Fine-tuning will be executed using the command python <trainingScript>. description: A concise overview of the model, highlighting its key features and capabilities. tokenizer: The root hash of the tokenizer files used for dataset processing. This value is obtained after uploading the tokenizer files to 0G storage. usageFile: The ZIP file (referenced by its name, not the full path) contains detailed usage information for this model, including training configuration examples, build specifications, or sample datasets. Make sure the file is placed in the ./models directory."
      },
      {
        "title": "Build the TDX Guest Image‚Äã",
        "content": "Prerequisites Installation‚Äã Install Docker: curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Add User to Docker Group: sudo usermod -aG docker $USER newgrp docker Verify Installation: docker --version docker run hello-world Build CVM Image‚Äã To ensure secure and private execution of fine-tuning tasks, you will build an image suitable for running in a Confidential Virtual Machine (CVM). This process leverages NVIDIA's TEE GPU technology and Intel CPUs with TDX support, enhancing security by running model training in an isolated environment. Clone Repository: git clone https://github.com/nearai/private-ml-sdk --recursive cd private-ml-sdk/ ./build.sh Image Files Location: Check out private-ml-sdk/images/. Available images include: dstack-nvidia-0.3.0: Production image without developer tools. dstack-nvidia-dev-0.3.0: Development image with tools like sshd, strace."
      },
      {
        "title": "Run Application‚Äã",
        "content": "Run the Local KMS‚Äã The Local KMS provides essential keys for CVM initialization, derived from local TEE hardware. Launch KMS: cd private-ml-sdk/meta-dstack-nvidia/dstack/key-provider-build/ ./run.sh Run the TDX Guest Image‚Äã Ensure you have a TDX host machine with the TDX driver and a compatible NVIDIA GPU. Update PATH: pushd private-ml-sdk/meta-dstack-nvidia/scripts/bin PATH=$PATH:`pwd` popd List Available GPUs: dstack lsgpu Create CVM Instance: Replace #PORT# with your configured port: dstack new docker-compose.yaml -o my-gpu-cvm --local-key-provider --gpu [GPU_ID] --image images/dstack-nvidia-0.3.0 -c 2 -m 4G -d 100G --port tcp:0.0.0.0:#PORT#:#PORT# Run the CVM‚Äã Copy Config File: cp config.local.yaml private-ml-sdk/my-gpu-cvm/shared/config.local.yaml Start the CVM: sudo -E dstack run my-gpu-cvm"
      },
      {
        "title": "Troubleshooting‚Äã",
        "content": "CVM fails to start Service not accessible Model upload issues By following these steps, you will successfully set up your service as a fine-tuning provider on the 0G Compute Network, leveraging secure and verifiable computing environments."
      }
    ]
  },
  {
    "menu": "0G Storage SDKs",
    "sections": [
      {
        "title": "Available SDKs‚Äã",
        "content": "Go SDK: Ideal for backend systems and applications built with Go TypeScript SDK: Perfect for frontend development and JavaScript-based projects"
      },
      {
        "title": "Core Features‚Äã",
        "content": "Both SDKs provide a streamlined interface to interact with the 0G Storage network: Upload and Download Files: Securely store and retrieve data of various sizes and formats Manage Data: List uploaded files, check their status, and control access permissions Leverage Decentralization: Benefit from the 0G network's distributed architecture for enhanced data availability, immutability, and censorship resistance"
      },
      {
        "title": "Quick Start Resources‚Äã",
        "content": "STARTER KITS AVAILABLE Get up and running quickly with our comprehensive starter kits: TypeScript Starter Kit - Complete examples with Express.js server and CLI tool Go Starter Kit - Ready-to-use examples with Gin server and CLI tool Both repositories include working examples, API documentation, and everything you need to start building. Go SDK TypeScript SDK Installation‚Äã Install the 0G Storage Client library: go get github.com/0glabs/0g-storage-client Setup‚Äã Import Required Packages‚Äã import ( \"context\" \"github.com/0glabs/0g-storage-client/common/blockchain\" \"github.com/0glabs/0g-storage-client/indexer\" \"github.com/0glabs/0g-storage-client/transfer\" \"github.com/0glabs/0g-storage-client/core\" ) Initialize Clients‚Äã Create the necessary clients to interact with the network: // Create Web3 client for blockchain interactions w3client := blockchain.MustNewWeb3(evmRpc, privateKey) defer w3client.Close() // Create indexer client for node management indexerClient, err := indexer.NewClient(indRpc) if err != nil { // Handle error } Parameters: evmRpc: 0G Chain RPC endpoint (e.g., https://evmrpc-testnet.0g.ai/) privateKey: Your Ethereum private key for signing transactions indRpc: Indexer RPC endpoint (e.g., https://indexer-storage-testnet-turbo.0g.ai) Core Operations‚Äã Node Selection‚Äã Select storage nodes before performing file operations: nodes, err := indexerClient.SelectNodes(ctx, segmentNumber, expectedReplicas, excludedNodes) if err != nil { // Handle error } Parameters: ctx: Context for operation management segmentNumber: Identifies which storage segment to use expectedReplicas: Number of file copies to maintain (minimum 1) excludedNodes: List of nodes to exclude from selection File Upload‚Äã Upload files to the network: // Create uploader uploader, err := transfer.NewUploader(ctx, w3client, nodes) if err != nil { // Handle error } // Upload file txHash, err := uploader.UploadFile(ctx, filePath) if err != nil { // Handle error } Parameters: ctx: Context for upload operation w3client: Web3 client instance nodes: Selected storage nodes filePath: Path to the file being uploaded File Hash Calculation‚Äã Calculate a file's Merkle root hash for identification: rootHash, err := core.MerkleRoot(filePath) if err != nil { // Handle error } fmt.Printf(\"File hash: %s \", rootHash.String()) IMPORTANT Save the root hash - you'll need it to download the file later! File Download‚Äã Download files from the network: // Create downloader downloader, err := transfer.NewDownloader(nodes) if err != nil { // Handle error } // Download with optional verification err = downloader.Download(ctx, rootHash, outputPath, withProof) if err != nil { // Handle error } Parameters: ctx: Context for download operation rootHash: File's unique identifier (Merkle root hash) outputPath: Where to save the downloaded file withProof: Enable/disable Merkle proof verification (true/false) Best Practices‚Äã Error Handling: Implement proper error handling and cleanup Context Management: Use contexts for operation timeouts and cancellation Resource Cleanup: Always close clients when done using defer client.Close() Verification: Enable proof verification for sensitive files Monitoring: Track transaction status for important uploads Additional Resources‚Äã Go SDK Repository Go Starter Kit Need more control? Consider running your own storage node to participate in the network and earn rewards."
      },
      {
        "title": "Installation‚Äã",
        "content": "Install the 0G Storage Client library: go get github.com/0glabs/0g-storage-client"
      },
      {
        "title": "Setup‚Äã",
        "content": "Import Required Packages‚Äã import ( \"context\" \"github.com/0glabs/0g-storage-client/common/blockchain\" \"github.com/0glabs/0g-storage-client/indexer\" \"github.com/0glabs/0g-storage-client/transfer\" \"github.com/0glabs/0g-storage-client/core\" ) Initialize Clients‚Äã Create the necessary clients to interact with the network: // Create Web3 client for blockchain interactions w3client := blockchain.MustNewWeb3(evmRpc, privateKey) defer w3client.Close() // Create indexer client for node management indexerClient, err := indexer.NewClient(indRpc) if err != nil { // Handle error } Parameters: evmRpc: 0G Chain RPC endpoint (e.g., https://evmrpc-testnet.0g.ai/) privateKey: Your Ethereum private key for signing transactions indRpc: Indexer RPC endpoint (e.g., https://indexer-storage-testnet-turbo.0g.ai)"
      },
      {
        "title": "Core Operations‚Äã",
        "content": "Node Selection‚Äã Select storage nodes before performing file operations: nodes, err := indexerClient.SelectNodes(ctx, segmentNumber, expectedReplicas, excludedNodes) if err != nil { // Handle error } Parameters: ctx: Context for operation management segmentNumber: Identifies which storage segment to use expectedReplicas: Number of file copies to maintain (minimum 1) excludedNodes: List of nodes to exclude from selection File Upload‚Äã Upload files to the network: // Create uploader uploader, err := transfer.NewUploader(ctx, w3client, nodes) if err != nil { // Handle error } // Upload file txHash, err := uploader.UploadFile(ctx, filePath) if err != nil { // Handle error } Parameters: ctx: Context for upload operation w3client: Web3 client instance nodes: Selected storage nodes filePath: Path to the file being uploaded File Hash Calculation‚Äã Calculate a file's Merkle root hash for identification: rootHash, err := core.MerkleRoot(filePath) if err != nil { // Handle error } fmt.Printf(\"File hash: %s \", rootHash.String()) IMPORTANT Save the root hash - you'll need it to download the file later! File Download‚Äã Download files from the network: // Create downloader downloader, err := transfer.NewDownloader(nodes) if err != nil { // Handle error } // Download with optional verification err = downloader.Download(ctx, rootHash, outputPath, withProof) if err != nil { // Handle error } Parameters: ctx: Context for download operation rootHash: File's unique identifier (Merkle root hash) outputPath: Where to save the downloaded file withProof: Enable/disable Merkle proof verification (true/false)"
      },
      {
        "title": "Best Practices‚Äã",
        "content": "Error Handling: Implement proper error handling and cleanup Context Management: Use contexts for operation timeouts and cancellation Resource Cleanup: Always close clients when done using defer client.Close() Verification: Enable proof verification for sensitive files Monitoring: Track transaction status for important uploads"
      },
      {
        "title": "Additional Resources‚Äã",
        "content": "Go SDK Repository Go Starter Kit"
      },
      {
        "title": "Installation‚Äã",
        "content": "Install the SDK and its peer dependency: npm install @0glabs/0g-ts-sdk ethers noteethers is a required peer dependency for blockchain interactions"
      },
      {
        "title": "Setup‚Äã",
        "content": "Import Required Modules‚Äã import { ZgFile, Indexer, Batcher, KvClient } from '@0glabs/0g-ts-sdk';import { ethers } from 'ethers'; Initialize Configuration‚Äã // Network Constantsconst RPC_URL = 'https://evmrpc-testnet.0g.ai/';const INDEXER_RPC = 'https://indexer-storage-testnet-turbo.0g.ai';// Initialize provider and signerconst privateKey = 'YOUR_PRIVATE_KEY'; // Replace with your private keyconst provider = new ethers.JsonRpcProvider(RPC_URL);const signer = new ethers.Wallet(privateKey, provider);// Initialize indexerconst indexer = new Indexer(INDEXER_RPC);"
      },
      {
        "title": "Core Operations‚Äã",
        "content": "File Upload‚Äã Complete upload workflow: async function uploadFile(filePath) { // Create file object from file path const file = await ZgFile.fromFilePath(filePath); // Generate Merkle tree for verification const [tree, treeErr] = await file.merkleTree(); if (treeErr !== null) { throw new Error(`Error generating Merkle tree: ${treeErr}`); } // Get root hash for future reference console.log(\"File Root Hash:\", tree?.rootHash()); // Upload to network const [tx, uploadErr] = await indexer.upload(file, RPC_URL, signer); if (uploadErr !== null) { throw new Error(`Upload error: ${uploadErr}`); } console.log(\"Upload successful! Transaction:\", tx); // Always close the file when done await file.close(); return { rootHash: tree?.rootHash(), txHash: tx };} File Download‚Äã Download with optional verification: async function downloadFile(rootHash, outputPath) { // withProof = true enables Merkle proof verification const err = await indexer.download(rootHash, outputPath, true); if (err !== null) { throw new Error(`Download error: ${err}`); } console.log(\"Download successful!\");} Key-Value Storage‚Äã Store and retrieve key-value data: // Upload data to 0G-KVasync function uploadToKV(streamId, key, value) { const [nodes, err] = await indexer.selectNodes(1); if (err !== null) { throw new Error(`Error selecting nodes: ${err}`); } const batcher = new Batcher(1, nodes, flowContract, RPC_URL); const keyBytes = Uint8Array.from(Buffer.from(key, 'utf-8')); const valueBytes = Uint8Array.from(Buffer.from(value, 'utf-8')); batcher.streamDataBuilder.set(streamId, keyBytes, valueBytes); const [tx, batchErr] = await batcher.exec(); if (batchErr !== null) { throw new Error(`Batch execution error: ${batchErr}`); } console.log(\"KV upload successful! TX:\", tx);}// Download data from 0G-KVasync function downloadFromKV(streamId, key) { const kvClient = new KvClient(\"http://3.101.147.150:6789\"); const keyBytes = Uint8Array.from(Buffer.from(key, 'utf-8')); const value = await kvClient.getValue(streamId, ethers.encodeBase64(keyBytes)); return value;} Browser Support‚Äã For browser environments, use the ESM build: <script type=\"module\"> import { Blob, Indexer } from \"./dist/zgstorage.esm.js\"; // Create file object from blob const file = new Blob(blob); const [tree, err] = await file.merkleTree(); if (err === null) { console.log(\"File Root Hash:\", tree.rootHash()); }</script> Stream Support‚Äã Work with streams for efficient data handling: import { Readable } from 'stream';// Upload from streamasync function uploadStream() { const stream = new Readable(); stream.push('Hello, 0G Storage!'); stream.push(null); const file = await ZgFile.fromStream(stream, 'hello.txt'); const [tx, err] = await indexer.upload(file, RPC_URL, signer); if (err === null) { console.log(\"Stream uploaded!\"); }}// Download as streamasync function downloadStream(rootHash) { const stream = await indexer.downloadFileAsStream(rootHash); stream.pipe(fs.createWriteStream('output.txt'));}"
      },
      {
        "title": "Best Practices‚Äã",
        "content": "Initialize Once: Create the indexer once and reuse it for multiple operations Handle Errors: Always implement proper error handling for network issues Use Appropriate Methods: Use ZgFile.fromFilePath for Node.js and Blob for browsers Secure Keys: Never expose private keys in client-side code Close Resources: Remember to call file.close() after operations"
      },
      {
        "title": "Additional Resources‚Äã",
        "content": "TypeScript SDK Repository TypeScript Starter Kit"
      }
    ]
  },
  {
    "menu": "0G Storage CLI",
    "sections": [
      {
        "title": "Why Use the CLI?‚Äã",
        "content": "Direct Control: Manage data location and versioning with precision Automation Ready: Build scripts and cron jobs for regular operations Full Feature Access: Access all storage and KV operations from the terminal Developer Friendly: Perfect for integrating into your development workflow WEB-BASED ALTERNATIVE For a quick and easy web interface, try the 0G Storage Web Tool - perfect for one-off uploads and downloads."
      },
      {
        "title": "Installation‚Äã",
        "content": "Prerequisites‚Äã Go 1.18 or higher installed on your system Git for cloning the repository Setup Steps‚Äã 1. Clone the Repository git clone https://github.com/0glabs/0g-storage-client.git cd 0g-storage-client 2. Build the Binary go build 3. Add to PATH (Optional but recommended) # Move binary to Go bin directory mv 0g-storage-client ~/go/bin # Add to PATH if not already configured export PATH=~/go/bin:$PATH"
      },
      {
        "title": "Command Overview‚Äã",
        "content": "The CLI provides a comprehensive set of commands for storage operations: 0g-storage-client [command] [flags] Available Commands: upload Upload file to 0G Storage network download Download file from 0G Storage network kv-write Write to KV streams kv-read Read KV streams gen Generate test files gateway Start gateway service indexer Start indexer service completion Generate shell completion scripts help Get help for any command Global Flags: --gas-limit uint Custom gas limit for transactions --gas-price uint Custom gas price for transactions --log-level string Set log level (default \"info\") --log-color-disabled Disable colorful log output --web3-log-enabled Enable Web3 RPC logging"
      },
      {
        "title": "Core Operations‚Äã",
        "content": "File Upload‚Äã Upload files to the 0G Storage network: 0g-storage-client upload --url <blockchain_rpc_endpoint> --contract <log_contract_address> --key <private_key> --node <storage_node_rpc_endpoint> --file <file_path> Parameters: --url: 0G Chain RPC endpoint (e.g., https://evmrpc-testnet.0g.ai) --contract: 0G log contract address on the blockchain --key: Your private key for signing transactions --node: Storage node RPC endpoint (e.g., https://rpc-storage-testnet.0g.ai) --file: Path to the file you want to upload File Download‚Äã Download files from the network: 0g-storage-client download --node <storage_node_rpc_endpoint> --root <file_root_hash> --file <output_file_path> Parameters: --node: Storage node RPC endpoint --root: File's Merkle root hash (obtained during upload) --file: Where to save the downloaded file Download with Verification‚Äã Enable proof verification for enhanced security: 0g-storage-client download --node <storage_node_rpc_endpoint> --root <file_root_hash> --file <output_file_path> --proof The --proof flag requests cryptographic proof of data integrity from the storage node."
      },
      {
        "title": "Practical Examples‚Äã",
        "content": "Upload Example‚Äã # Upload a document to 0G Storage 0g-storage-client upload --url https://evmrpc-testnet.0g.ai --contract 0x8873cc79c5b3b5666535C825205C9a128B1D75F1 --key YOUR_PRIVATE_KEY --node https://rpc-storage-testnet.0g.ai --file ./documents/report.pdf # Output: # ‚úì File uploaded successfully # Root hash: 0xc5d2460186f7233c927e7db2dcc703c0e500b653ca82273b7bfad8045d85a470 # Transaction: 0x742d35cc6634c0532925a3b844bc454e8e4a0e3f... Download Example‚Äã # Download file using root hash 0g-storage-client download --node https://rpc-storage-testnet.0g.ai --root 0xc5d2460186f7233c927e7db2dcc703c0e500b653ca82273b7bfad8045d85a470 --file ./downloads/report.pdf # With verification 0g-storage-client download --node https://rpc-storage-testnet.0g.ai --root 0xc5d2460186f7233c927e7db2dcc703c0e500b653ca82273b7bfad8045d85a470 --file ./downloads/report.pdf --proof"
      },
      {
        "title": "Key-Value Operations‚Äã",
        "content": "Write to KV Store‚Äã 0g-storage-client kv-write --url <blockchain_rpc_endpoint> --contract <kv_contract_address> --key <private_key> --stream-id <stream_id> --stream-key <key> --stream-value <value> Read from KV Store‚Äã 0g-storage-client kv-read --node <kv_node_endpoint> --stream-id <stream_id> --stream-key <key>"
      },
      {
        "title": "Advanced Features‚Äã",
        "content": "Custom Gas Settings‚Äã Control transaction costs with custom gas parameters: 0g-storage-client upload --gas-limit 3000000 --gas-price 10000000000 # ... other parameters Logging Configuration‚Äã Adjust logging for debugging: # Verbose logging with Web3 details 0g-storage-client upload --log-level debug --web3-log-enabled # ... other parameters # Minimal logging 0g-storage-client download --log-level error --log-color-disabled # ... other parameters Shell Completion‚Äã Enable tab completion for easier command entry: # Bash 0g-storage-client completion bash > /etc/bash_completion.d/0g-storage-client # Zsh 0g-storage-client completion zsh > \"${fpath[1]}/_0g-storage-client\" # Fish 0g-storage-client completion fish > ~/.config/fish/completions/0g-storage-client.fish"
      },
      {
        "title": "Important Considerations‚Äã",
        "content": "Network Configuration‚Äã REQUIRED INFORMATION Contract Addresses: Find the latest contract addresses in the 0G documentation or on the Storage Explorer Node Endpoints: Use team-provided endpoints or run your own storage node Private Keys: Keep your private keys secure and never share them File Management‚Äã Root Hash Storage: Save file root hashes after upload - they're required for downloads Transaction Monitoring: Track upload transactions on the blockchain explorer Node Selection: Choose reliable storage nodes or run your own for better control"
      },
      {
        "title": "Running Services‚Äã",
        "content": "Indexer Service‚Äã The indexer helps users find suitable storage nodes: 0g-storage-client indexer --listen :8080 --node <storage_node_endpoint> Gateway Service‚Äã Run a gateway to provide HTTP access to storage: 0g-storage-client gateway --listen :9000 --node <storage_node_endpoint>"
      },
      {
        "title": "Automation Examples‚Äã",
        "content": "Backup Script‚Äã Create automated backup scripts: #!/bin/bash # backup.sh - Daily backup to 0G Storage DATE=$(date +%Y%m%d) BACKUP_FILE=\"/backups/daily-${DATE}.tar.gz\" # Create backup tar -czf $BACKUP_FILE /important/data # Upload to 0G ROOT_HASH=$(0g-storage-client upload --url $RPC_URL --contract $CONTRACT --key $PRIVATE_KEY --node $NODE_URL --file $BACKUP_FILE | grep \"Root hash\" | cut -d' ' -f3) # Save root hash echo \"${DATE}: ${ROOT_HASH}\" >> /backups/manifest.txt Monitoring Integration‚Äã Monitor uploads with logging: # upload-with-monitoring.sh 0g-storage-client upload --file $1 --log-level info # ... other parameters 2>&1 | tee -a /var/log/0g-uploads.log"
      },
      {
        "title": "Troubleshooting‚Äã",
        "content": "Upload fails with \"insufficient funds\" error \"Node not found\" error during download"
      },
      {
        "title": "Best Practices‚Äã",
        "content": "Security First: Store private keys in environment variables, not command line Backup Root Hashes: Always save file root hashes after uploads Use Verification: Enable --proof for important downloads Monitor Transactions: Track uploads on the blockchain explorer Test First: Use testnet before mainnet operations Need more control? Consider running your own storage node to participate in the network and earn rewards."
      }
    ]
  },
  {
    "menu": "0G Data Availability (DA): Integration",
    "sections": [
      {
        "title": "Overview‚Äã",
        "content": "Maximum Blob Size‚Äã Users can submit data blobs up to 32,505,852 bytes in length, which are then processed, encoded, and distributed across a network of DA nodes. The system employs a sophisticated data processing flow that includes padding, matrix formation, redundant encoding, and signature aggregation. Fee Market‚Äã As the DA user, you pay a fee which is the (BLOB_PRICE) when submitting DA blob data. Submitting Data‚Äã See example here: https://github.com/0glabs/0g-da-example-rust/blob/main/src/disperser.proto"
      },
      {
        "title": "Hardware Requirements‚Äã",
        "content": "The following table outlines the hardware requirements for different types of DA Client nodes: Node Type Memory CPU Disk Bandwidth Additional Notes DA Client 8 GB 2 cores - 100 MBps For Download / Upload DA Encoder - - - - NVIDIA Drivers: 12.04 on the RTX 4090* DA Retriever 8 GB 2 cores - 100 MBps For Download / Upload"
      },
      {
        "title": "Standing up DA Client, Encoder, Retriever‚Äã",
        "content": "DA Client DA Encoder DA Retriever DA Client Node Installation‚Äã 1. Clone the DA Client Node Repo git clone https://github.com/0glabs/0g-da-client.git 2. Build the Docker Image cd 0g-da-client docker build -t 0g-da-client -f combined.Dockerfile . 3. Set Environment Variables Create a file named envfile.env with the following content. Be sure you paste in your private key. # envfile.env COMBINED_SERVER_CHAIN_RPC=https://evmrpc-testnet.0g.ai COMBINED_SERVER_PRIVATE_KEY=YOUR_PRIVATE_KEY ENTRANCE_CONTRACT_ADDR=0x857C0A28A8634614BB2C96039Cf4a20AFF709Aa9 COMBINED_SERVER_RECEIPT_POLLING_ROUNDS=180 COMBINED_SERVER_RECEIPT_POLLING_INTERVAL=1s COMBINED_SERVER_TX_GAS_LIMIT=2000000 COMBINED_SERVER_USE_MEMORY_DB=true COMBINED_SERVER_KV_DB_PATH=/runtime/ COMBINED_SERVER_TimeToExpire=2592000 DISPERSER_SERVER_GRPC_PORT=51001 BATCHER_DASIGNERS_CONTRACT_ADDRESS=0x0000000000000000000000000000000000001000 BATCHER_FINALIZER_INTERVAL=20s BATCHER_CONFIRMER_NUM=3 BATCHER_MAX_NUM_RETRIES_PER_BLOB=3 BATCHER_FINALIZED_BLOCK_COUNT=50 BATCHER_BATCH_SIZE_LIMIT=500 BATCHER_ENCODING_INTERVAL=3s BATCHER_ENCODING_REQUEST_QUEUE_SIZE=1 BATCHER_PULL_INTERVAL=10s BATCHER_SIGNING_INTERVAL=3s BATCHER_SIGNED_PULL_INTERVAL=20s BATCHER_EXPIRATION_POLL_INTERVAL=3600 BATCHER_ENCODER_ADDRESS=DA_ENCODER_SERVER BATCHER_ENCODING_TIMEOUT=300s BATCHER_SIGNING_TIMEOUT=60s BATCHER_CHAIN_READ_TIMEOUT=12s BATCHER_CHAIN_WRITE_TIMEOUT=13s 4. Run the Docker Node docker run -d --env-file envfile.env --name 0g-da-client -v ./run:/runtime -p 51001:51001 0g-da-client combined Configuration‚Äã Field Description --chain.rpc JSON RPC node endpoint for the blockchain network. --chain.private-key Hex-encoded signer private key. --chain.receipt-wait-rounds Maximum retries to wait for transaction receipt. --chain.receipt-wait-interval Interval between retries when waiting for transaction receipt. --chain.gas-limit Transaction gas limit. --combined-server.use-memory-db Whether to use mem-db for blob storage. --combined-server.storage.kv-db-path Path for level db. --combined-server.storage.time-to-expire Expiration duration for blobs in level db. --combined-server.log.level-file File log level. --combined-server.log.level-std Standard output log level. --combined-server.log.path Log file path. --disperser-server.grpc-port Server listening port. --disperser-server.retriever-address GRPC host for retriever. --batcher.da-entrance-contract Hex-encoded da-entrance contract address. --batcher.da-signers-contract Hex-encoded da-signers contract address. --batcher.finalizer-interval Interval for finalizing operations. --batcher.finalized-block-count Default number of blocks between finalized block and latest block. --batcher.confirmer-num Number of Confirmer threads. --batcher.max-num-retries-for-sign Number of retries before signing fails. --batcher.batch-size-limit Maximum batch size in MiB. --batcher.encoding-request-queue-size Size of the encoding request queue. --batcher.encoding-interval Interval between blob encoding requests. --batcher.pull-interval Interval for pulling from the encoded queue. --batcher.signing-interval Interval between slice signing requests. --batcher.signed-pull-interval Interval for pulling from the signed queue. --encoder-socket GRPC host of the encoder. --encoding-timeout Total time to wait for a response from encoder. --signing-timeout Total time to wait for a response from signer."
      },
      {
        "title": "DA Client Node Installation‚Äã",
        "content": "1. Clone the DA Client Node Repo git clone https://github.com/0glabs/0g-da-client.git 2. Build the Docker Image cd 0g-da-client docker build -t 0g-da-client -f combined.Dockerfile . 3. Set Environment Variables Create a file named envfile.env with the following content. Be sure you paste in your private key. # envfile.env COMBINED_SERVER_CHAIN_RPC=https://evmrpc-testnet.0g.ai COMBINED_SERVER_PRIVATE_KEY=YOUR_PRIVATE_KEY ENTRANCE_CONTRACT_ADDR=0x857C0A28A8634614BB2C96039Cf4a20AFF709Aa9 COMBINED_SERVER_RECEIPT_POLLING_ROUNDS=180 COMBINED_SERVER_RECEIPT_POLLING_INTERVAL=1s COMBINED_SERVER_TX_GAS_LIMIT=2000000 COMBINED_SERVER_USE_MEMORY_DB=true COMBINED_SERVER_KV_DB_PATH=/runtime/ COMBINED_SERVER_TimeToExpire=2592000 DISPERSER_SERVER_GRPC_PORT=51001 BATCHER_DASIGNERS_CONTRACT_ADDRESS=0x0000000000000000000000000000000000001000 BATCHER_FINALIZER_INTERVAL=20s BATCHER_CONFIRMER_NUM=3 BATCHER_MAX_NUM_RETRIES_PER_BLOB=3 BATCHER_FINALIZED_BLOCK_COUNT=50 BATCHER_BATCH_SIZE_LIMIT=500 BATCHER_ENCODING_INTERVAL=3s BATCHER_ENCODING_REQUEST_QUEUE_SIZE=1 BATCHER_PULL_INTERVAL=10s BATCHER_SIGNING_INTERVAL=3s BATCHER_SIGNED_PULL_INTERVAL=20s BATCHER_EXPIRATION_POLL_INTERVAL=3600 BATCHER_ENCODER_ADDRESS=DA_ENCODER_SERVER BATCHER_ENCODING_TIMEOUT=300s BATCHER_SIGNING_TIMEOUT=60s BATCHER_CHAIN_READ_TIMEOUT=12s BATCHER_CHAIN_WRITE_TIMEOUT=13s 4. Run the Docker Node docker run -d --env-file envfile.env --name 0g-da-client -v ./run:/runtime -p 51001:51001 0g-da-client combined"
      },
      {
        "title": "Configuration‚Äã",
        "content": "Field Description --chain.rpc JSON RPC node endpoint for the blockchain network. --chain.private-key Hex-encoded signer private key. --chain.receipt-wait-rounds Maximum retries to wait for transaction receipt. --chain.receipt-wait-interval Interval between retries when waiting for transaction receipt. --chain.gas-limit Transaction gas limit. --combined-server.use-memory-db Whether to use mem-db for blob storage. --combined-server.storage.kv-db-path Path for level db. --combined-server.storage.time-to-expire Expiration duration for blobs in level db. --combined-server.log.level-file File log level. --combined-server.log.level-std Standard output log level. --combined-server.log.path Log file path. --disperser-server.grpc-port Server listening port. --disperser-server.retriever-address GRPC host for retriever. --batcher.da-entrance-contract Hex-encoded da-entrance contract address. --batcher.da-signers-contract Hex-encoded da-signers contract address. --batcher.finalizer-interval Interval for finalizing operations. --batcher.finalized-block-count Default number of blocks between finalized block and latest block. --batcher.confirmer-num Number of Confirmer threads. --batcher.max-num-retries-for-sign Number of retries before signing fails. --batcher.batch-size-limit Maximum batch size in MiB. --batcher.encoding-request-queue-size Size of the encoding request queue. --batcher.encoding-interval Interval between blob encoding requests. --batcher.pull-interval Interval for pulling from the encoded queue. --batcher.signing-interval Interval between slice signing requests. --batcher.signed-pull-interval Interval for pulling from the signed queue. --encoder-socket GRPC host of the encoder. --encoding-timeout Total time to wait for a response from encoder. --signing-timeout Total time to wait for a response from signer."
      },
      {
        "title": "Features‚Äã",
        "content": "parallel: Uses parallel algorithms for computations, maximizing CPU resource utilization. cuda: Uses GPU for computations, applicable only on platforms with NVIDIA GPUs. noteGPU support is currently tested with NVIDIA 12.04 drivers on the RTX 4090. Other NVIDIA GPUs may require parameter adjustments and have not been tuned yet."
      },
      {
        "title": "Preparation‚Äã",
        "content": "Install Rust‚Äã Ensure you have curl installed. Run the following command to install Rust: curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh After installation, add the cargo bin directory to your PATH environment variable: source $HOME/.cargo/env Verify the installation: rustc --version Install other dependencies‚Äã # Install Protocol Buffers Compilersudo apt-get install -y protobuf-compiler# Install a specific nightly Rust toolchain and rustfmtrustup toolchain install nightly-2024-02-04-x86_64-unknown-linux-gnurustup component add --toolchain nightly-2024-02-04-x86_64-unknown-linux-gnu rustfmt# Add the necessary Rust targetrustup target add x86_64-unknown-linux-gnu Install CUDA (for GPU feature)‚Äã Ensure you have an NVIDIA GPU with the required drivers. Then follow the instructions from CUDA Toolkit. Verify the installation: nvidia-sminvcc --version"
      },
      {
        "title": "Building Public Parameters‚Äã",
        "content": "The public parameters for the cryptographic protocol are built in two steps: 1. Download and process the perpetual power of tau‚Äã We use the challenge_0084 file from the nearly most recent submission. curl https://pse-trusted-setup-ppot.s3.eu-central-1.amazonaws.com/challenge_0084 -o challenge_0084 2. Build the AMT parameters‚Äã You can either construct these parameters yourself or download pre-built files. Choice 1: Download the pre-built files‚Äã ./dev-support/download_params.sh Choice 2: Construct the parameters yourself‚Äã ./dev_support/build_params.sh challenge_0084"
      },
      {
        "title": "Running the Server‚Äã",
        "content": "Run the server with the following command: cargo run -r -p server --features grpc/parallel,grpc/cuda -- --config run/config.toml noteIf you do not have a CUDA environment, remove the cuda feature. DA Encoder will serve on port 34000 with specified gRPC interface."
      },
      {
        "title": "Using the Verification Logic‚Äã",
        "content": "Add the following to Cargo.toml of your crate: zg-encoder = { git = \"https://github.com/0glabs/0g-da-encoder.git\" } Use the zg_encoder::EncodedSlice::verify function for verifying."
      },
      {
        "title": "Benchmark the Performance‚Äã",
        "content": "Run the following task: cargo bench -p grpc --features grpc/parallel,grpc/cuda --bench process_data --features zg-encoder/production_mode -- --nocapture"
      },
      {
        "title": "Development and Testing‚Äã",
        "content": "Run the following script for complete testing: ./dev_support/test.sh"
      },
      {
        "title": "DA Retriever Node Installation‚Äã",
        "content": "1. Clone the DA Retriever Node Repo git clone https://github.com/0glabs/0g-da-retriever.gitcd 0g-da-retriever 2. Edit Files Add the following line to Dockerfile.dockerignore file: !/run/config.toml Replace Dockerfile with the following: # DockerfileFROM rust:alpine3.20 as builderWORKDIR /0g-da-retrieverCOPY . .RUN apk update && apk add --no-cache make protobuf-dev musl-devRUN cargo build --releaseFROM alpine:3.20WORKDIR /0g-da-retrieverCOPY --from=builder /0g-da-retriever/target/release/retriever /usr/local/bin/retriever# Copy the config file into the containerCOPY --from=builder /0g-da-retriever/run/config.toml ./run/config.toml# Set the entrypoint to run the retriever binaryCMD [\"/usr/local/bin/retriever\"] Replace the Config impl in /retriever/src/config.rs with the following: impl Config { pub fn from_cli_file() -> Result<Self> { let matches = cli::cli_app().get_matches(); let config_file = matches .get_one::<String>(\"config\") .map(|s| s.as_str()) .unwrap_or(\"/0g-da-retriever/run/config.toml\"); let c = RawConfig( config::Config::builder() .add_source(config::File::with_name(config_file)) .build()?, ); Ok(Self { log_level: c.get_string(\"log_level\")?, eth_rpc_url: c.get_string(\"eth_rpc_endpoint\")?, grpc_listen_address: c.get_string(\"grpc_listen_address\")?, max_ongoing_retrieve_request: c.get_u64_opt(\"max_ongoing_retrieve_request\")?, }) }} 3. Update Configuration Update configuration file run/config.toml as needed with context below. FieldDescriptionlog_levelSet log level.grpc_listen_addressServer listening address.eth_rpc_endpointJSON RPC node endpoint for the blockchain network. 4. Build and Run the Docker Node docker build -t 0g-da-retriever . docker run -d --name 0g-da-retriever -p 34005:34005 0g-da-retriever"
      },
      {
        "title": "Troubleshooting‚Äã",
        "content": "DA Client connection issues Encoder GPU not detected Retriever fails to start"
      },
      {
        "title": "Next Steps‚Äã",
        "content": "Integration Examples ‚Üí DA Examples Repository Join Community ‚Üí Discord for support Run a DA Node ‚Üí DA Node Guide Ready to integrate 0G DA into your application? Start with the DA Client and connect to the network."
      }
    ]
  },
  {
    "menu": "0G DA Technical Deep Dive",
    "sections": [
      {
        "title": "DA Processing Flow‚Äã",
        "content": "DA takes an input of data up to 32,505,852 bytes in length and processes it as follows: Padding and Size Encoding: Pad the data with zeros until it reaches 32,505,852 bytes Add a little-endian format 4-byte integer at the end to indicate the original input size Matrix Formation: Slice the padded data into a 1024-row by 1024-column matrix, filling each row consecutively, with each element being 31 bytes Pad each 31-byte element with an additional 1-byte zero, making it 32 bytes per element Redundant Encoding: Expand the data to a 3072-row by 1024-column matrix using redundancy coding Calculate the erasure commitment and data root of the expanded matrix Submission to DA Contract: Submit the erasure commitment and data root to the DA contract and pay the fee The DA contract will determine the epoch to which the data belongs and assign a quorum Data Distribution: Send the erasure commitment, data root, each row of the matrix, and necessary proofs of correctness to the corresponding DA nodes Signature Aggregation: More than 2/3 of the DA nodes sign the erasure commitment and data root Aggregate the signatures using the BLS signature algorithm and submit the aggregated signature to the DA contract Details of Erasure Encoding‚Äã After matrix formation, each element is processed into a 32-byte data unit, which can be viewed interchangeably as either 32 bytes of data or a 256-bit little-endian integer. Denote the element in the ùëñ i-th row and ùëó j-th column as ùëê ùëñ , ùëó c i,j ‚Äã . Let the finite field ùêπ F be the scalar field of the BN254 curve. Each element ùëê ùëñ , ùëó c i,j ‚Äã is also considered an integer within the finite field ùêπ F. Let ùëù p be the order of ùêπ F, a known large number that can be expressed as 2 28 √ó ùê¥ + 1 2 28 √óA+1, where ùê¥ A is an odd number. The number 3 is a generator of the multiplicative group of the ùêπ F. Define ùë¢ = 3 2 6 √ó ùê¥ u=3 2 6 √óA and ùë§ = 3 2 8 √ó ùê¥ w=3 2 8 √óA , so ùë§ 2 20 = 1 w 2 20 =1 and ùë¢ 4 = ùë§ u 4 =w. Now we define a polynomial ùëì f over ùêπ ‚Üí ùêπ F‚ÜíF with degree ùëë = 2 20 ‚àí 1 d=2 20 ‚àí1 satisfying ‚àÄ 0 ‚â§ ùëñ < 1024 , 0 ‚â§ ùëó < 1024 , ùëì ( ùë§ 1024 ùëó + ùëñ ) = ùëê ùëñ , ùëó ‚àÄ0‚â§i<1024,0‚â§j<1024,f(w 1024j+i )=c i,j ‚Äã Then we extend the 1024 √ó 1024 1024√ó1024 matrix into 1024 √ó 3072 1024√ó3072 matrix, where ‚àÄ 1024 ‚â§ ùëñ < 2048 , 0 ‚â§ ùëó < 1024 , ùëê ùëñ , ùëó = ùëì ( ùë¢ 2 ‚ãÖ ùë§ 1024 ùëó + ( ùëñ ‚àí 1024 ) ) ‚àÄ1024‚â§i<2048,0‚â§j<1024,c i,j ‚Äã =f(u 2 ‚ãÖw 1024j+(i‚àí1024) ) ‚àÄ 2048 ‚â§ ùëñ < 3072 , 0 ‚â§ ùëó < 1024 , ùëê ùëñ , ùëó = ùëì ( ùë¢ ‚ãÖ ùë§ 1024 ùëó + ( ùëñ ‚àí 2048 ) ) ‚àÄ2048‚â§i<3072,0‚â§j<1024,c i,j ‚Äã =f(u‚ãÖw 1024j+(i‚àí2048) ) The erasure commitment is the KZG commitment of ùëì f, defined as ùëì ( ùúè ) ‚ãÖ ùê∫ f(œÑ)‚ãÖG, where ùê∫ G is the starting point of BN254 G1 curve, and ùúè œÑ is a latent parameter from the perpetual powers of tau trusted setup ceremony. The data root is defined as the input root by treating the 1024*3072 32-byte elements as a continuous storage submission input. Specifically, according to the storage submission requirements, these data does not need to pad any zeros, and will be divided into a 16384-element sector array and an 8192-element sector array. DA nodes need to verify two parts: The consistency between the received slice and the data root, mainly achieved through Merkle proofs The consistency between the received slice and the erasure commitment, verified using KZG proofs. Here, we use the AMT protocol optimization introduced in LVMT to reduce the proving overhead"
      },
      {
        "title": "DA Sampling‚Äã",
        "content": "The blockchain will periodically release DA Sampling tasks at preset height every SAMPLE_PERIOD blocks, with the parent block hash of these heights used as the sampleSeed for DA Sampling. List of Parameters‚Äã Constant parameters Parameter Requirement Default value MAX_PODAS_TARGET 2^256 / 128 - 1 Admin adjustable parameters Parameter Requirement Default value Code TARGET_SUBMITS 20 Link EPOCH_WINDOW_SIZE 300 (about 3 months) Link SAMPLE_PERIOD 30 (about 1.5 minutes) Link Responses‚Äã During each period, each DA slice (one row) can be divided into 32 sub-lines. For each sub-line, the podasQuality will be computed using the dataRoot and assigned epoch and quorumId of its corresponding DA blob. NOTE By default, all integers are in 256-bit big-endian format when computing hash values. lineIndex is the only exception, which is in 64-bit big-endian format. The hash value can be viewed interchangeably as either 32 bytes of data or a 256-bit big-endian integer. lineQuality = keccak256(sampleSeed, epoch, quorumId, dataRoot, lineIndex); dataQuality = keccak256(lineQuality, sublineIndex, data); podasQuality = lineQuality + dataQuality If the podasQuality is less than the current podasTarget in the DA contract and the epoch falls within [currentEpoch - EPOCH_WINDOW_SIZE, currentEpoch), then this sub-line is regarded as a valid DAS response and is eligible for the reward. The DA node assigned to this row can claim the reward. During a sample period, at most TARGET_SUBMITS √ó 2 DAS responses can be submitted and rewarded; any submissions exceeding this limit will be rejected. Difficulty Adjustment‚Äã TARGET_SUBMITS valid responses are expected in a sample period. If more or fewer responses are submitted during a sample period, the podasTarget will be adjusted as follows: podasTarget -= podasTarget * (actualSubmits - TARGET_SUBMITS) / TARGET_SUBMITS / 8"
      },
      {
        "title": "Economic Model‚Äã",
        "content": "List of Parameters‚Äã Admin adjustable parameters Parameter Requirement Default value Code BASE_REWARD 0 Link BLOB_PRICE 0 Link SERVICE_FEE_RATE_BP 0 Link REWARD_RATIO [1] 1,200,000 Link [1] TARGET_SUBMITS √ó Time elapsed for EPOCH_WINDOW_SIZE epochs / Time elapsed in SAMPLE_PERIOD / REWARD_RATIO should be approximately 0.5 to 2. Pricing‚Äã When users submit the metadata for a DA blob, they need to pay a fee in amount of BLOB_PRICE. Reward‚Äã When a DA epoch ends, all the rewards from that DA epoch will be stored in the DA reward pool. Each time a valid response is submitted, 1 / REWARD_RATIO of the reward pool will be distributed to the corresponding DA node. System Rewards‚Äã In the early stages of the ecosystem, the foundation can reserve a portion of tokens for system rewards. When the DA node submits a valid response, an additional reward of BASE_REWARD will be issued. The funds for the base reward will be manually deposited into the reward contract and tracked separately. If the balance for the base reward is insufficient to cover a single base reward, miners will not be able to receive the full base reward. Service Fee‚Äã A system service fee is charged as a proportion of the DA fees paid by the user, according to the parameter SERVICE_FEE_RATE_BP."
      },
      {
        "title": "Run a Node‚Äã",
        "content": "See here for instructions to become DA signer and run your own node. Ready to dive deeper into 0G DA? Join our Discord for technical discussions."
      }
    ]
  },
  {
    "menu": "Overview",
    "sections": []
  },
  {
    "menu": "Validator Node",
    "sections": [
      {
        "title": "Hardware Requirements‚Äã",
        "content": "Component Mainnet Testnet Memory 64 GB 64 GB CPU 8 cores 8 cores Disk 1 TB NVME SSD 4 TB NVME SSD Bandwidth 100 MBps for Download / Upload 100 MBps for Download / Upload"
      },
      {
        "title": "Setup Guide‚Äã",
        "content": "1. Download Package‚Äã Download the latest package for node binaries: wget -O galileo.tar.gz https://github.com/0glabs/0gchain-NG/releases/download/v1.2.0/galileo-v1.2.0.tar.gz 2. Extract Package‚Äã Extract the package to your home directory: tar -xzvf galileo.tar.gz -C ~ 3. Copy Files and Set Permissions‚Äã Copy the configuration files and set proper permissions: cd galileo cp -r 0g-home {your data path} sudo chmod 777 ./bin/geth sudo chmod 777 ./bin/0gchaind 4. Initialize Geth‚Äã Initialize the Geth client with the genesis file: ./bin/geth init --datadir /{your data path}/0g-home/geth-home ./genesis.json 5. Initialize 0gchaind‚Äã Create a temporary directory for initial configuration: ./bin/0gchaind init {node name} --home /{your data path}/tmp 6. Copy Node Files‚Äã Move the generated keys to the proper location: cp /{your data path}/tmp/data/priv_validator_state.json /{your data path}/0g-home/0gchaind-home/data/ cp /{your data path}/tmp/config/node_key.json /{your data path}/0g-home/0gchaind-home/config/ cp /{your data path}/tmp/config/priv_validator_key.json /{your data path}/0g-home/0gchaind-home/config/ Note: The temporary directory can be deleted after this step. 7. Start 0gchaind‚Äã cd ~/galileo nohup ./bin/0gchaind start --rpc.laddr tcp://0.0.0.0:26657 --chaincfg.chain-spec devnet --chaincfg.kzg.trusted-setup-path=kzg-trusted-setup.json --chaincfg.engine.jwt-secret-path=jwt-secret.hex --chaincfg.kzg.implementation=crate-crypto/go-kzg-4844 --chaincfg.block-store-service.enabled --chaincfg.node-api.enabled --chaincfg.node-api.logging --chaincfg.node-api.address 0.0.0.0:3500 --pruning=nothing --home /{your data path}/0g-home/0gchaind-home --p2p.seeds 85a9b9a1b7fa0969704db2bc37f7c100855a75d9@8.218.88.60:26656 --p2p.external_address {your node ip}:26656 > /{your data path}/0g-home/log/0gchaind.log 2>&1 & 8. Start Geth‚Äã cd ~/galileo nohup ./bin/geth --config geth-config.toml --nat extip:{your node ip} --bootnodes enode://de7b86d8ac452b1413983049c20eafa2ea0851a3219c2cc12649b971c1677bd83fe24c5331e078471e52a94d95e8cde84cb9d866574fec957124e57ac6056699@8.218.88.60:30303 --datadir /{your data path}/0g-home/geth-home --networkid 16601 > /{your data path}/0g-home/log/geth.log 2>&1 & 9. Verify Setup‚Äã Check the logs to confirm your node is running properly: # Check Geth logs tail -f /{your data path}/0g-home/log/geth.log # Check 0gchaind logs tail -f /{your data path}/0g-home/log/0gchaind.log Check logs to confirm your node is running properly. SUCCESS INDICATORS 0gchaind should show \"Committed state\" messages No error messages in either log"
      },
      {
        "title": "Next Steps‚Äã",
        "content": "Staking Integration‚Äã Once your validator node is running, you can interact with the staking system programmatically using smart contracts: Staking Interfaces Guide - Complete documentation for integrating with 0G Chain staking smart contracts"
      }
    ]
  },
  {
    "menu": "Storage Node",
    "sections": [
      {
        "title": "Prerequisites‚Äã",
        "content": "Before setting up your storage node: Understand that 0G Storage interacts with on-chain contracts for blob root confirmation and PoRA mining. Check here for deployed contract addresses."
      },
      {
        "title": "Install Dependencies‚Äã",
        "content": "Start by installing all the essential tools and libraries required to build the 0G storage node software. Linux Mac sudo apt-get update sudo apt-get install clang cmake build-essential pkg-config libssl-dev Install rustup: rustup is the Rust toolchain installer, necessary as the 0G node software is written in Rust. curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh Download the Source Code: git clone -b <latest_tag> https://github.com/0glabs/0g-storage-node.git Build the Source Code cd 0g-storage-node # Build in release mode cargo build --release This compiles the Rust code into an executable binary. The --release flag optimizes the build for performance."
      },
      {
        "title": "Configuration‚Äã",
        "content": "Navigate to the run directory and open config.toml for editing. Follow the steps below. Edit the configuration file: cd run nano config.toml Update configuration with your preferred settings: Below is just an example configuration for illustration purposes. For official default values, copy over the config-testnet-turbo.toml file to your config.toml file. # Peer nodes: A list of peer nodes to help your node join the network. Check inside 0g-storage/run directory for suggested configurations. network_boot_nodes = [] # Contract addresses log_contract_address = \"CONTRACT_ADDRESS\" #flow contract address, see testnet information mine_contract_address = \"CONTRACT_ADDRESS\" #Address of the smart contract on the host blockchain that manages mining. # L1 host blockchain RPC endpoint URL. See testnet information page for RPC endpoints blockchain_rpc_endpoint = \"RPC_ENDPOINT\" # Start sync block number: The block number from which your node should start synchronizing the log data. log_sync_start_block_number = BLOCK_NUMBER # Your private key (64 chars, no '0x' prefix, include leading zeros): Your private key (without the `0x` prefix) if you want to participate in PoRA mining and earn rewards. miner_key = \"YOUR_PRIVATE_KEY\" # Max chunk entries in db (affects storage size): The maximum number of chunk entries (each 256 bytes) to store in the database. This effectively limits the database size. db_max_num_chunks = MAX_CHUNKS # ENR address: Your node's public IP address, essential for other nodes to discover and connect to you. Currently automatically set by the node. # network_enr_address ="
      },
      {
        "title": "Running the Storage Node‚Äã",
        "content": "Check configuration options: ../target/release/zgs_node -h Run the storage service: cd run ../target/release/zgs_node --config config.toml --miner-key <your_private_key>"
      },
      {
        "title": "Snapshot‚Äã",
        "content": "Make sure to only include flow_db and delete data_db under db folder when you use a snapshot from a 3rd party ! Using others' data_db will make the node mine for others! Additional Notes Security: Keep your private key (miner_key) safe and secure. Anyone with access to it can control your node and potentially claim your mining rewards. Network Connectivity: Ensure your node has a stable internet connection and that the necessary ports are open for communication with other nodes. Monitoring: Monitor your node's logs and resource usage to ensure it's running smoothly. Updates: Stay informed about updates to the 0G storage node software and follow the project's documentation for any changes in the setup process. Remember: Running a storage node is a valuable contribution to the 0G network. You'll be helping to maintain its decentralization and robustness while earning rewards for your efforts."
      },
      {
        "title": "Overview‚Äã",
        "content": "0G Storage KV is a key-value store built on top of the 0G Storage system. This guide provides detailed steps to deploy and run a 0G Storage KV node."
      },
      {
        "title": "Prerequisites‚Äã",
        "content": "Before setting up your 0G Storage KV node: Understand that 0G KV interacts with on-chain contracts and storage nodes to simulate KV data streams. For official deployed contract addresses, visit the testnet information page."
      },
      {
        "title": "Install Dependencies‚Äã",
        "content": "Follow the same steps to install dependencies and Rust as in the storage node setup: linuxmacsudo apt-get updatesudo apt-get install clang cmake build-essential pkg-config libssl-devbrew install llvm cmake Install rustup: rustup is the Rust toolchain installer, necessary as the 0G node software is written in Rust. curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh 1. Download the Source Code‚Äã git clone -b <latest_tag> https://github.com/0glabs/0g-storage-kv.git 2. Build the Source Code‚Äã cd 0g-storage-kv# Build in release modecargo build --release"
      },
      {
        "title": "Configuration‚Äã",
        "content": "Copy the example configuration file and update it: cp config_example.toml config.tomlnano config.toml Update the following fields in config.toml: ########################################################################## Key-Value Stream Options ########################################################################### Streams to monitor.stream_ids = [\"000000000000000000000000000000000000000000000000000000000000f2bd\", \"000000000000000000000000000000000000000000000000000000000000f009\", \"0000000000000000000000000000000000000000000000000000000000016879\", \"0000000000000000000000000000000000000000000000000000000000002e3d\"]########################################################################## DB Config Options ########################################################################### Directory to store data.db_dir = \"db\"# Directory to store KV Metadata.kv_db_dir = \"kv.DB\"########################################################################## Log Sync Config Options ##########################################################################blockchain_rpc_endpoint = \"BLOCKCHAIN_RPC_ENDPOINT\" #rpc endpoint, see testnet informationlog_contract_address = \"LOG_CONTRACT_ADDRESS\" #flow contract address, see testnet information# log_sync_start_block_number should be earlier than the block number of the first transaction that writes to the stream being monitored.log_sync_start_block_number = 0########################################################################## RPC Config Options ########################################################################### Whether to provide RPC service.rpc_enabled = true# HTTP server address to bind for public RPC.rpc_listen_address = \"0.0.0.0:6789\"# Zerog storage nodes to download data from.zgs_node_urls = \"http://127.0.0.1:5678,http://127.0.0.1:5679\"########################################################################## Misc Config Options ##########################################################################log_config_file = \"log_config"
      },
      {
        "title": "Running the Storage KV Node‚Äã",
        "content": "Navigate to the run directory: cd run Run the KV service: ../target/release/zgs_kv --config config.toml For long-running sessions, consider using tmux or screen to run the node in the background."
      },
      {
        "title": "Monitoring and Maintenance‚Äã",
        "content": "Check logs: The node outputs logs based on the log_config file specified in your configuration. Updating the node: To update to the latest version, pull the latest changes from the repository and rebuild: git pull cargo build --release"
      },
      {
        "title": "Troubleshooting‚Äã",
        "content": "If you encounter issues: Check the logs for any error messages. Ensure your node meets the hardware requirements. Verify that your config.toml file is correctly formatted and contains valid settings. Check your internet connection and firewall settings. Ensure the specified blockchain RPC endpoint and contract addresses are correct and accessible."
      },
      {
        "title": "Getting Help‚Äã",
        "content": "If you need assistance: Check the GitHub Issues for known problems and solutions. Join the 0G community channels (Discord, Telegram, etc.) for community support. For critical issues, consider reaching out to the 0G team directly."
      },
      {
        "title": "Conclusion‚Äã",
        "content": "Running a 0G Storage KV node is an important part of the 0G ecosystem, providing key-value storage capabilities. By following this guide, you should be able to set up and operate your node successfully. Remember to keep your node updated and monitor its performance regularly to ensure optimal operation."
      }
    ]
  },
  {
    "menu": "Data Availability Node",
    "sections": [
      {
        "title": "Hardware Requirements‚Äã",
        "content": "Node Type Memory CPU Disk Bandwidth Additional Notes DA Node 16 GB 8 cores 1 TB NVMe SSD 100 MBps For Download/Upload"
      },
      {
        "title": "Standing up a DA Node and DA Signer‚Äã",
        "content": "Run with Docker Build from Source Become a Signer 1. Clone the DA Node Repo: git clone https://github.com/0glabs/0g-da-node.git cd 0g-da-node 2. Generate BLS Private Key (if needed): If you don't have a BLS private key, generate one: cargo run --bin key-gen Keep the generated BLS private key secure. 3. Set up config.toml: Create a configuration file named config.toml in the project root directory. Add the following content to the file, adjusting values as needed: log_level = \"info\" data_path = \"/data\" # path to downloaded params folder encoder_params_dir = \"/params\" # grpc server listen address grpc_listen_address = \"0.0.0.0:34000\" # chain eth rpc endpoint eth_rpc_endpoint = \"https://evmrpc-testnet.0g.ai\" # public grpc service socket address to register in DA contract # ip:34000 (keep same port as the grpc listen address) # or if you have dns, fill your dns socket_address = \"<public_ip/dns>:34000\" # data availability contract to interact with da_entrance_address = \"0x857C0A28A8634614BB2C96039Cf4a20AFF709Aa9\" # testnet config # deployed block number of da entrance contract start_block_number = 940000 # testnet config # signer BLS private key signer_bls_private_key = \"\" # signer eth account private key signer_eth_private_key = \"\" # miner eth account private key, (could be the same as `signer_eth_private_key`, but not recommended) miner_eth_private_key = \"\" # whether to enable data availability sampling enable_das = \"true\" Make sure to fill in the signer_bls_private_key, signer_eth_private_key, and miner_eth_private_key fields with your actual private keys. 4. Build and Start the Docker Container: docker build -t 0g-da-node . docker run -d --name 0g-da-node 0g-da-node 5. Verify the Node is Running On the first run, the DA node will register the signer information in the DA contract. You can monitor the console output to ensure the node is running correctly and has successfully registered. Node Operations‚Äã As a DA node operator, your node will perform the following tasks: Encoded blob data verification Signing of verified data Storing blob data for further farming Receiving rewards for these operations Troubleshooting‚Äã If you encounter any issues, check the console output for error messages. Ensure that the ports specified in your config.toml file are not being used by other applications. Verify that you have the latest stable version of Rust installed. Make sure your system meets the minimum hardware requirements. Conclusion‚Äã You have now successfully set up and run a 0g DA node as a DA Signer. For more advanced configuration options and usage instructions, please refer to the Official GitHub repository. Remember to keep your private keys secure and regularly update your node software to ensure optimal performance and security."
      },
      {
        "title": "Step 1: Clone and Build the Repository‚Äã",
        "content": "Install dependencies: sudo apt-get update && sudo apt-get install clang cmake build-essential pkg-config libssl-dev protobuf-compiler llvm llvm-dev Clone the repository and checkout the specific version: git clone https://github.com/0glabs/0g-da-node.gitcd 0g-da-node Build the project: cargo build --release Download necessary parameters: ./dev_support/download_params.sh"
      },
      {
        "title": "Step 2: Generate BLS Private Key (if needed)‚Äã",
        "content": "If you don't have a BLS private key, generate one: cargo run --bin key-gen Keep the generated BLS private key secure."
      },
      {
        "title": "Step 3: Configure the Node‚Äã",
        "content": "Create a configuration file named config.toml in the project root directory. Add the following content to the file, adjusting values as needed: log_level = \"info\"data_path = \"./db/\"# path to downloaded params folderencoder_params_dir = \"params/\" # grpc server listen addressgrpc_listen_address = \"0.0.0.0:34000\"# chain eth rpc endpointeth_rpc_endpoint = \"https://evmrpc-testnet.0g.ai\"# public grpc service socket address to register in DA contract# ip:34000 (keep same port as the grpc listen address)# or if you have dns, fill your dnssocket_address = \"<public_ip/dns>:34000\"# data availability contract to interact withda_entrance_address = \"0x857C0A28A8634614BB2C96039Cf4a20AFF709Aa9\" # testnet config and see testnet page for the latest info# deployed block number of da entrance contractstart_block_number = 940000 # testnet config# signer BLS private keysigner_bls_private_key = \"\"# signer eth account private keysigner_eth_private_key = \"\"# miner eth account private key, (could be the same as `signer_eth_private_key`, but not recommended)miner_eth_private_key = \"\"# whether to enable data availability samplingenable_das = \"true\" Make sure to fill in the signer_bls_private_key, signer_eth_private_key, and miner_eth_private_key fields with your actual private keys."
      },
      {
        "title": "Step 4: Run the Node‚Äã",
        "content": "Start the 0g DA node using the following command: ./target/release/server --config config.toml This will start the node using the configuration file you created."
      },
      {
        "title": "Step 5: Verify the Node is Running‚Äã",
        "content": "On the first run, the DA node will register the signer information in the DA contract. You can monitor the console output to ensure the node is running correctly and has successfully registered."
      },
      {
        "title": "Node Operations‚Äã",
        "content": "As a DA node operator, your node will perform the following tasks: Encoded blob data verification Signing of verified data Storing blob data for further farming Receiving rewards for these operations"
      },
      {
        "title": "Troubleshooting‚Äã",
        "content": "If you encounter any issues, check the console output for error messages. Ensure that the ports specified in your config.toml file are not being used by other applications. Verify that you have the latest stable version of Rust installed. Make sure your system meets the minimum hardware requirements."
      },
      {
        "title": "Conclusion‚Äã",
        "content": "You have now successfully set up and run a 0g DA node as a DA Signer. For more advanced configuration options and usage instructions, please refer to the Official GitHub repository. Remember to keep your private keys secure and regularly update your node software to ensure optimal performance and security."
      },
      {
        "title": "Overview‚Äã",
        "content": "The DASigners contract is an interface through which Solidity contracts can interact with the 0G chain module DASigners. It is registered as a precompiled contract, similar to other precompiled EVM extensions."
      },
      {
        "title": "Becoming a DA Signer‚Äã",
        "content": "To become a DA signer, you must meet the following requirements: Delegation Requirement: To become a signer, an address must receive enough delegations, equivalent to at least the TokensPerVote amount of OG tokens (30 tokens per vote in the testnet), registered in the DASigners module. Node Operation: Each signer needs to run a DA (Data Availability) node that verifies blob encoding and generates BLS signatures for signed blobs. Registration: Signers must register their information using the registerSigner function. This includes providing their address, node socket address, BLS public key, and a signature signed by their BLS private key. Epoch Participation: Signers must submit a registration message (using the registerNextEpoch function) with a signature for each epoch they wish to participate in. This is necessary for joining quorums in the next epoch. Voting Power: Each signer‚Äôs voting power is determined by the number of tokens delegated to them. Signers can have up to 1024 votes, and the votes are distributed randomly into quorums. Quorum Responsibilities: Each signer in a quorum is responsible for validating, signing, and storing a specific row of encoded blob data during an epoch."
      },
      {
        "title": "Prerequisites‚Äã",
        "content": "Ensure you have the following installed on your system: Git Rust (latest stable version) Cargo (comes with Rust)"
      },
      {
        "title": "Contract Details‚Äã",
        "content": "Address: 0x0000000000000000000000000000000000001000 Contract Params (Testnet)‚Äã TokensPerVote = 30MaxVotesPerSigner = 1024MaxQuorums = 10EpochBlocks = 5760EncodedSlices = 3072"
      },
      {
        "title": "Terminology‚Äã",
        "content": "Signer‚Äã A Signer is an address with sufficient delegations (at least TokensPerVote OG) registered in the DASigners module. Each signer should run a DA node to verify DA blob encoding and generate BLS signatures for signed blobs. The BLS curve used is BN254, and the public keys of signers are registered in the contract. Note: For accounts with delegations to more than 10 validators, only 10 of these delegations are counted and accumulated. Epoch‚Äã The consecutive blocks in the 0g chain are divided into groups of EpochBlocks, and each group is an epoch. Quorum‚Äã In an epoch, there can be up to MaxQuorums quorums. Each quorum is a list of signer addresses with size EncodedSlices. The i-th signer in the quorum is responsible for validating, signing, and storing the i-th row of the encoded blob data assigned to this quorum. Vote‚Äã Signers can submit their signatures on a registration message to request joining the quorums in the next epoch. At the start of each epoch, the DASigners module calculates the voting power for registered signers based on their delegated token amounts. Each delegated TokensPerVote OG counts as one vote, and each signer can have up to MaxVotesPerSigner votes. All votes are then randomly ordered and distributed into quorums."
      },
      {
        "title": "Interface‚Äã",
        "content": "Find the Solidity interface in the 0g-da-contract repo."
      },
      {
        "title": "ABI‚Äã",
        "content": "Find the ABI in the 0g-chain repo."
      },
      {
        "title": "Transactions‚Äã",
        "content": "registerSigner‚Äã Register signer's information, including signer address, DA node service socket address, signer BLS public key on G1 and G2 group, and a signature signed by the BLS private key of the following message: Keccak256(signerAddress, chainID, \"0G_BN254_Pubkey_Registration\") Here chainID is left-padded to 32 bytes by zeros. function registerSigner( SignerDetail memory _signer, BN254.G1Point memory _signature) external; updateSocket‚Äã Update signer's socket address. function updateSocket(string memory _socket) external; registerNextEpoch‚Äã Register to join the quorums in the next epoch. The signer needs to submit a signature signed by their BLS private key: Keccak256(signerAddress, epoch, chainID) Here chainID is left-padded to 32 bytes by zeros and epoch is an unsigned 64-bit number in big-endian format. function registerNextEpoch(BN254.G1Point memory _signature) external;"
      },
      {
        "title": "Queries‚Äã",
        "content": "epochNumber‚Äã Get the current epoch number. function epochNumber() external view returns (uint); quorumCount‚Äã Get the number of quorums for a given epoch. function quorumCount(uint _epoch) external view returns (uint); isSigner‚Äã Check if a given address is a registered signer. function isSigner(address _account) external view returns (bool); getSigner‚Äã Get the information of given signers. function getSigner(address[] memory _account) external view returns (SignerDetail[] memory); getQuorum‚Äã Get the signer list of a given epoch and quorum id. function getQuorum(uint _epoch, uint _quorumId) external view returns (address[] memory); getQuorumRow‚Äã Get the signer of a specific row in a given epoch and quorum id. function getQuorumRow(uint _epoch, uint _quorumId, uint32 _rowIndex) external view returns (address); registeredEpoch‚Äã Check if a given address is registered to join the given epoch. function registeredEpoch(address _account, uint _epoch) external view returns (bool); getAggPkG1‚Äã Get the aggregated G1 public key for a given signers set. The signers set is specified by the epoch, quorum id, and a bitmap. The bitmap has EncodedSlices bits, and each bit denotes whether the row is chosen or not. function getAggPkG1( uint _epoch, uint _quorumId, bytes memory _quorumBitmap) external view returns (BN254.G1Point memory aggPkG1, uint total, uint hit);"
      }
    ]
  },
  {
    "menu": "Community Docker Repository",
    "sections": []
  },
  {
    "menu": "Security at 0G",
    "sections": [
      {
        "title": "Audits‚Äã",
        "content": "We regularly conduct thorough security audits of our smart contracts, protocols, and infrastructure to ensure the highest level of security for our users. Recent Audits‚Äã Date Auditor Scope Report Aug 2024 - Sept 2024 Halborn 0G Storage Report Aug 2024 Zellic 0G Storage and 0G DA Report For a complete list of our audits and their detailed reports, please visit our GitHub repository."
      },
      {
        "title": "0G Labs Bug Bounty Program with Hackenproof‚Äã",
        "content": "At 0G, we believe in the power of community-driven security. Our bug bounty program invites security researchers and developers to help us identify and resolve potential vulnerabilities, ensuring the robustness of our systems. Scope of the Bug Bounty Program‚Äã Our bug bounty program covers: Smart Contracts Infrastructure Protocol"
      },
      {
        "title": "Focus Area‚Äã",
        "content": "In-Scope Vulnerabilities:‚Äã We are interested in vulnerabilities that result in incorrect behavior of the smart contract and could lead to unintended functionality, including: Stealing or loss of funds Unauthorized transactions Transaction manipulation Attacks on logic (behavior that deviates from the intended business logic) Reentrancy attacks Reordering transactions Overflows and underflows Out-of-Scope Vulnerabilities:‚Äã The following are out of scope for the bug bounty program: Theoretical vulnerabilities without proof or demonstration Old compiler versions Unlocked compiler version Vulnerabilities in imported contracts Code style guide violations Redundant code Gas optimizations Best practice issues Vulnerabilities exploitable through front-run attacks only Additionally, the following contracts are out of scope for 0g-storage-contract: cashier token reward/OnePoolReward reward/ChunkDecayReward uploadMarket utils/Exponent.sol Rewards‚Äã Rewards are based on the severity of the discovered vulnerability: Severity Reward Range Critical $35,000 High $8000 Medium $2000 Low $500 Program Rules‚Äã Avoid using web application scanners for automatic vulnerability searching which generates massive traffic Make every effort not to damage or restrict the availability of products, services, or infrastructure Avoid compromising any personal data, interruption, or degradation of any service Don‚Äôt access or modify other user data, localize all tests to your accounts Perform testing only within the scope Don‚Äôt exploit any DoS/DDoS vulnerabilities, social engineering attacks, or spam Don‚Äôt spam forms or account creation flows using automated scanners In case you find chain vulnerabilities we‚Äôll pay only for vulnerability with the highest severity. Don‚Äôt break any law and stay in the defined scope Any details of found vulnerabilities must not be communicated to anyone who is not a HackenProof Team or an authorized employee of this Company without appropriate permission Disclosure Guidelines‚Äã IMPORTANT Do not discuss this program or any vulnerabilities (even resolved ones) outside of the program without express consent from the organization No vulnerability disclosure, including partial is allowed for the moment. Please do NOT publish/discuss bugs Eligibility and Coordinated Disclosure‚Äã We are happy to thank everyone who submits valid reports which help us improve the security. However, only those that meet the following eligibility requirements may receive a monetary reward: You must be the first reporter of a vulnerability. The vulnerability must be a qualifying vulnerability Any vulnerability found must be reported no later than 24 hours after discovery and exclusively through hackenproof.com You must send a clear textual description of the report along with steps to reproduce the issue, include attachments such as screenshots or proof of concept code as necessary. You must not be a former or current employee of us or one of its contractors. ONLY USE the EMAIL under which you registered your HackenProof account (in case of violation, no bounty can be awarded) Provide detailed but to-the point reproduction steps We look forward to working with the community to enhance 0G's security!"
      }
    ]
  },
  {
    "menu": "Contribute to 0G Blockchain",
    "sections": []
  },
  {
    "menu": "Glossary",
    "sections": [
      {
        "title": "A‚Äã",
        "content": "AI Agent: An autonomous software program that can perceive its environment, make decisions, and take actions to achieve specific goals. AVS (Actively Validated Services): Services that require active validation from operators, commonly used in restaking protocols like EigenLayer and Babylon."
      },
      {
        "title": "C‚Äã",
        "content": "Chain: In the context of 0G, refers to the 0G blockchain that serves as the foundational layer for transactions and smart contracts. Compute Network: The distributed network of nodes that provide computational resources for AI workloads including inference and training."
      },
      {
        "title": "D‚Äã",
        "content": "DA (Data Availability): A layer that ensures data required by blockchain applications is available when needed, crucial for scalability and security. deAIOS: Decentralized AI Operating System - 0G's comprehensive infrastructure for decentralized AI applications. Decentralized Storage: A storage system that distributes data across multiple nodes rather than relying on centralized servers."
      },
      {
        "title": "E‚Äã",
        "content": "ERC-721: The standard interface for non-fungible tokens (NFTs) on Ethereum and EVM-compatible chains. ERC-7857: An extension of ERC-721 that adds support for encrypted metadata, enabling secure transfer of AI agents as NFTs. Erasure Coding: A data protection method that breaks data into fragments and encodes it with redundant pieces to ensure recovery even if some parts are lost."
      },
      {
        "title": "I‚Äã",
        "content": "Inference: The process of using a trained AI model to make predictions or decisions based on new input data. INFT (Intelligent Non-Fungible Token): NFTs that can encapsulate AI agents with their intelligence and capabilities intact."
      },
      {
        "title": "M‚Äã",
        "content": "Modular Blockchain: A blockchain architecture where different functions (consensus, execution, data availability) are separated into specialized layers."
      },
      {
        "title": "O‚Äã",
        "content": "Oracle: In the context of INFTs, a service that verifies the integrity of metadata transfers using either TEE or ZKP technology."
      },
      {
        "title": "P‚Äã",
        "content": "Precompile: Built-in functions in a blockchain that are implemented at the protocol level for optimal performance. Proof of Random Access (PoRA): 0G's consensus mechanism that ensures data availability by requiring nodes to prove they can access random data samples."
      },
      {
        "title": "Q‚Äã",
        "content": "Quorum: A minimum number of nodes required to reach consensus on a decision in a distributed system."
      },
      {
        "title": "R‚Äã",
        "content": "RaaS (Rollup as a Service): Platforms that provide infrastructure and tools to easily deploy and manage blockchain rollups. Rollup: A scaling solution that processes transactions off the main chain while posting transaction data back to it."
      },
      {
        "title": "S‚Äã",
        "content": "Sharding: A scaling technique that divides a network into smaller parts (shards) to process transactions in parallel. Storage Node: A node in the 0G network that stores and serves data to the network."
      },
      {
        "title": "T‚Äã",
        "content": "TEE (Trusted Execution Environment): A secure area of a processor that ensures code and data loaded inside are protected with respect to confidentiality and integrity. Testnet: A test network that mimics the main network but uses test tokens, allowing developers to experiment without real value at risk."
      },
      {
        "title": "V‚Äã",
        "content": "Validator Node: A node that participates in consensus by validating transactions and proposing new blocks."
      },
      {
        "title": "W‚Äã",
        "content": "Web3: The vision of a decentralized internet built on blockchain technology, emphasizing user ownership and control."
      },
      {
        "title": "Z‚Äã",
        "content": "Zero Gravity (0G): The name representing the weightless state where transactions and data exchanges occur effortlessly in the 0G ecosystem. Zero-Knowledge Proof (ZKP): A cryptographic method where one party can prove to another that a statement is true without revealing any information beyond the validity of the statement. This glossary is continuously updated as the 0G ecosystem evolves. If you encounter a term not listed here, please contribute by submitting a pull request to our documentation repository."
      }
    ]
  },
  {
    "menu": "Introduction",
    "sections": [
      {
        "title": "Purpose and Benefits of the 0G AI Alignment Node Sale‚Äã",
        "content": "The objective of 0G Foundation: The 0G Foundation‚Äôs node sale aims to create a decentralized AI operating system (deAIOS) that operates transparently, safely, and under community influence. Centralized AI structures may lack these attributes, creating potential risks for data integrity and security. The 0G Foundation‚Äôs goal is to develop AI as a public good, fostering an ecosystem that prioritizes transparency and reduces reliance on central authorities, making it suitable for broad public utility."
      },
      {
        "title": "What is an AI Alignment Node?‚Äã",
        "content": "Alignment nodes provide the key utility of monitoring whether the other kinds of decentralized nodes in the 0G network - validator nodes, storage nodes, security nodes - faithfully follow network protocols. To start, the nodes will have certain utility, and in the near future, AI Alignment Nodes will have additional utility to monitor on-chain AI model drift and to ensure that 0G‚Äôs on-chain AI is behaving as intended. To sustain the node utility operation and allow the network to be further secured, Alignment Node owners may receive a portion of the fees collected by the network by operating the nodes and contributing work to the 0G network. Node sale participants and stakers (i.e., long term believers who secure the ecosystem) may get additional rewards from the 0G ecosystem over time. The 0G node sale will be a launch that enables the community to participate on an equal playing field with entry points within multiple tiers."
      },
      {
        "title": "Why Run a Node?‚Äã",
        "content": "Running a node on the 0G network offers participants a chance to directly contribute to the growth and security of our decentralized AI ecosystem. Nodes are the backbone of our network, enabling the following: Decentralization: Ensure the network remains decentralized and resilient. Network Security: Enhance the security and reliability of the network by verifying transactions. AI Processing Power: Support AI computations, thereby contributing to the network's ability to deliver on-chain AI services. Reward Opportunities: Node operators can earn rewards in the form of native tokens or other incentives for their contribution to the network."
      },
      {
        "title": "What are the specific use cases for AI Alignment Nodes?‚Äã",
        "content": "Protocol Monitoring: Ensuring that validator, storage, and security nodes comply with network protocols. AI Model Monitoring: Tracking AI model alignment and checking for any unintended behavior. Network Security: Safeguarding the ecosystem by flagging deviations and ensuring consistent ethical standards. Economic and Governance Roles: Node holders may earn rewards and participate in governance decisions, influencing the network‚Äôs direction."
      }
    ]
  },
  {
    "menu": "Node Holder Benefits",
    "sections": []
  },
  {
    "menu": "Sale Structure, Dates, and Tiers",
    "sections": [
      {
        "title": "Sale Phases‚Äã",
        "content": "The sale is structured into two phases: Whitelist Sale: Whitelist Sale opens November 11, 2024, at 12 PM UTC and remains open for two days. The sale will be denominated in USDC on Arbitrum, and only whitelisted participants may purchase nodes during this phase. Public Sale opens November 13, 2024, at 12 PM UTC, is denominated in USDC on Arbitrum, and is available to all users, subject to geographic and regulatory restrictions. Please see our disclaimer for more information."
      },
      {
        "title": "Tier Pricing‚Äã",
        "content": "The node sale is segmented into 32 pricing tiers, beginning at 0.05 ETH per node, allowing for varied entry points that cater to diverse participant levels. While pegged to an ETH snapshot price of $3,130, both sales are conducted in USDC on Arbitrum. See here for more details."
      }
    ]
  },
  {
    "menu": "Eligibility and Whitelist Participation",
    "sections": [
      {
        "title": "Disclaimer‚Äã‚Äã",
        "content": "Please review the full details and terms of the 0G Node Sale by visiting our disclaimer."
      }
    ]
  },
  {
    "menu": "Purchasing Nodes: Steps and Payment Options",
    "sections": [
      {
        "title": "Video Tutorial‚Äã",
        "content": "IMPORTANT Please note: Initially, the Public Sale was intended to use wETH on Arbitrum, but after the community‚Äôs feedback, it will be USDC on Arbitrum. Please note this correction to USDC on Arbitrum as the video says wETH. Note that this video is for demonstrative purposes only, may not reflect exact details and have different prices or tokens than the actual sale."
      },
      {
        "title": "Step-by-Step Purchase Process:‚Äã",
        "content": "Step 1 - Go to https://node.0gfoundation.ai and connect your wallet‚Äã Step 2 - Select the chain and token you want to purchase‚Äã Step 3 - View the tiers available and purchase‚Äã In total there are 32 available for purchase. If you hold a whitelisted address, the Tier that you are entitled to will be shown on the top. Sold out tiers will be moved to the bottom of the page. Step 4 - Set the quantity‚Äã Start by inputting the number of node(s) that you will be purchasing. Step 5 - Input Promo Code (Optional)‚Äã If you have a Promo Code, input before your purchase. The discount rate will be applied directly to your checkout price. Additionally, if you have already purchased a node, you can provide your wallet address as a referral promo code too! This will provide a 10% rebate to the referral, and 10% commission if they purchase successfully. Step 6 - Approve transaction‚Äã Once you are confirmed on the final price, click the \"Approve\" button. You are then required to approve the transaction via your wallet. Step 7 - Complete purchase‚Äã Continue by clicking \"Purchase\". You will need to confirm the transaction via your wallet. After confirming your purchase, you will receive a notification on the top right of the page to inform you of the successful purchase."
      }
    ]
  },
  {
    "menu": "Incentives, Rewards, and Vesting Mechanisms",
    "sections": [
      {
        "title": "Rebate/Commission Portal Tutorial‚Äã",
        "content": ""
      },
      {
        "title": "How can node buyers create a referral code?‚Äã",
        "content": "Node buyers will be able to share their wallet address as the referral code after they made a purchase, it would give a 10% rebate to their referrals."
      },
      {
        "title": "What rebate is available when using a referral code?‚Äã",
        "content": "If you enter a referral code when purchasing a node, you'll receive a 10% rebate on the total price."
      },
      {
        "title": "How will I receive my commission? (For Referrers)‚Äã‚Äã",
        "content": "You will be able to claim the commission of any successful node purchased through the node on the reward claim site, which will be available after the public sale. Please refer to the 0G X for access to the claim site closer to the sale date."
      },
      {
        "title": "Vesting Terms‚Äã",
        "content": "Rewards from node operation vest over a three-year schedule, promoting consistent and long-term engagement. Vesting reduces the likelihood of short-term sales, fostering network stability and growth."
      },
      {
        "title": "Additional Community Incentives‚Äã",
        "content": "Partnerships, 0G ecosystem communities, and referrals provide extra benefits, such as rebates or promotional whitelist access. All promotional activities adhere to regulatory guidelines, and any reward or referral payments are conditional upon KYC eligibility."
      }
    ]
  },
  {
    "menu": "Compliance and Regulatory Requirements",
    "sections": [
      {
        "title": "Regulation S Compliance‚Äã",
        "content": "The sale follows Regulation S guidelines, restricting U.S. persons from participating. The sale website, promotional content, and user interface clearly indicate these restrictions, and KYC verification is mandatory for claiming rewards to maintain regulatory adherence. Prospective participants are advised to review these conditions and understand that resale of node licenses is not permitted within the first 12 months."
      },
      {
        "title": "Information Disclosure‚Äã",
        "content": "All communications related to the sale are made with transparency but exclude any directed selling to U.S. persons. Information shared in marketing materials, promotional activities, and social media avoids U.S.-targeted content, aligning with compliance requirements to mitigate any regulatory risks."
      }
    ]
  },
  {
    "menu": "Frequently Asked Questions",
    "sections": [
      {
        "title": "Whitelist & Node Sale‚Äã",
        "content": "What is the whitelist (WL)?‚Äã‚Äã Whitelist is a pre-approved list of participants who are given exclusive access to certain privileges during a sale event. This system is used to reward and incentivize key contributors, partners, or early supporters of a project. Does entering the whitelist guarantee that I can definitely purchase a node?‚Äã‚Äã Your whitelist guarantees an allocation during the Whitelist sale period (starting Nov 11). If a purchase is not made within this period, your allocation will be released. How to join the whitelist?‚Äã‚Äã To get a whitelist spot for the 0G Foundation Node Sale, community members and ecosystem participants are eligible for allocations, please visit 0G X and Discord for ways to receive a whitelist. You can also apply for a whitelist spot by filling out the whitelist form here."
      },
      {
        "title": "Payment & Licenses‚Äã",
        "content": "What payment methods will be accepted for purchasing a node?‚Äã‚Äã The nodes will be priced in USDC for both the Whitelist Sale and Public Sale, both on Arbitrum. However, to facilitate payment for users on different chains, the AI Alignment Node Sale will be accepting multiple tokens across multiple networks through a live bridging aggregator that accepts multichain payment including but not limited to BTC, ETH, ARB, SOL, etc. More info here. How will the node licenses be distributed?‚Äã‚Äã After the node sale period is completed, node licenses will be distributed as an NFT to the purchase wallet of the user. What will I receive from participating in the node sale?‚Äã‚Äã You will receive a soulbound NFT (ERC-721) which represents your Node License. The NFTs can be minted and transferred to your wallet via claim.0gfoundation.ai after the conclusion of the node sale. You will be able to operate the node after 0G Mainnet is live. Will the NFTs be transferable?‚Äã The Alignment Node license gives buyers lifetime access. The NFTs will be non-transferable for the first year after the node sale."
      },
      {
        "title": "Node Operations‚Äã",
        "content": "What are the hardware requirements?‚Äã‚Äã 0G Foundation‚Äôs AI Alignment nodes are designed for adoption - they can be run on community member's laptops, desktops, mobiles, or even on cloud instances. As for device requirements, the configuration needed is very minimal: 64MB RAM 1 x86 CPU Core @2.1GHz 10GB Disk Space 10Mbps Internet Connection When can I begin operating my node?‚Äã‚Äã AI Alignment utility will go live in 2025, after 0G Mainnet Launch. How many nodes can I purchase?‚Äã‚Äã The number of purchasable nodes will be capped per tier. Please refer to the Node Sale Tier documentation for reference. How do I run a node? Is it complicated?‚Äã‚Äã Running a node can be quite straightforward and easy, typically involving just a few steps. Here's a video tutorial to guide you through the process: If you prefer not to manage the node yourself, you can delegate to other node operators with just a single click through our explorer, which will be available shortly. Will the sale be accessible from other platforms?‚Äã‚Äã Both Public and Whitelist sale will be available here except for Partnered Launchpads, in which case the front end will be on 0G Partners' website. What will happen to any unsold node rewards?‚Äã Rewards from unsold nodes will be reallocated to the sold node runners. What is your tokenomics?‚Äã What is the % unlocked at TGE?‚Äã 33% of node rewards will be initially claimable on TGE. In order to encourage long term participation in the 0G ecosystem, there will be a penalty based on duration in which a participant chooses to receive this initial reward. The remaining 67% of alignment rewards are linearly unlocked (daily) over 36 months. This penalty mechanism is subject to community vote, further showcasing the \"community first approach\" of 0G. When is KYC required? How is it conducted?‚Äã KYC is required before claiming rewards. It will be provided by a third-party provider. No refund will be offered if the node purchaser does not meet KYC requirements. How many nodes can I purchase?‚Äã Each individual can purchase up to a certain amount of nodes per tier. See here for the caps per tier. What happened to unsold nodes?‚Äã Unsold alignment node NFT licenses will be burned. The rewards from the unsold nodes will be re-allocated to the sold verified node buyers. What are the launchpad partners working on the 0G Foundation Node Sale?‚Äã"
      }
    ]
  }
]