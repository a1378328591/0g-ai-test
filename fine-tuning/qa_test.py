from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

# åŠ è½½ fine-tuned æ¨¡åž‹ï¼ˆè¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰
model_path = "qa_model"  # ä½ è®­ç»ƒåŽä¿å­˜æ¨¡åž‹çš„ç›®å½•
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForQuestionAnswering.from_pretrained(model_path)

def answer_question(question: str, context: str) -> str:
    inputs = tokenizer(
        question,
        context,
        return_tensors="pt",
        truncation=True,
        padding=True,
        return_offsets_mapping=True
    )

    # offset_mapping æ˜¯æŽ¨ç†ä¸­ä¸éœ€è¦çš„ï¼Œåˆ é™¤é¿å…æŠ¥é”™
    if "offset_mapping" in inputs:
        del inputs["offset_mapping"]

    with torch.no_grad():
        outputs = model(**inputs)

    start_logits = outputs.start_logits
    end_logits = outputs.end_logits

    start_index = torch.argmax(start_logits, dim=1).item()
    end_index = torch.argmax(end_logits, dim=1).item()

    # æ£€æŸ¥åˆç†æ€§ï¼šstart å¿…é¡»å°äºŽç­‰äºŽ end ä¸”é•¿åº¦åˆç†
    if start_index > end_index or end_index - start_index > 30:
        return "(æ¨¡åž‹æœªèƒ½æ‰¾åˆ°åˆé€‚ç­”æ¡ˆ)"

    input_ids = inputs["input_ids"][0]
    answer_ids = input_ids[start_index:end_index + 1]
    answer = tokenizer.decode(answer_ids, skip_special_tokens=True)

    return answer.strip()

# ðŸ§ª äº¤äº’æ¨¡å¼æµ‹è¯•
if __name__ == "__main__":
    print("ðŸ“˜ Input English context and question. Press Ctrl+C to exit.")
    try:
        while True:
            context = input("\nEnter context:\n> ").strip()
            if not context:
                print("âš ï¸ Context cannot be empty.")
                continue

            while True:
                question = input("\nEnter question (empty to re-enter context):\n> ").strip()
                if not question:
                    break

                answer = answer_question(question, context)
                print(f"âœ… Answer: {answer}")
    except KeyboardInterrupt:
        print("\nðŸ‘‹ Exiting.")
